{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATH&ML-4 Математический анализ в контексте задачи оптимизации. Часть 2\n",
    "###  Содержание <a class=\"anchor\" id=0></a>\n",
    "\n",
    "- [1. Функции нескольких переменных](#2)\n",
    "- [2. Частные производные](#2)\n",
    "- [3. Безусловные экстремумы](#3)\n",
    "- [4. Введение в оптимизацию](#4)\n",
    "- [5. Условные экстремумы. Метод Лагранжа](#5)\n",
    "- [6. Градиент и антиградиент](#6)\n",
    "- [7. Градиент и градиентный спуск](#7)\n",
    "- [8. Практика. Градиентный спуск](#8)\n",
    "- [9. Итоги](#8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Функции нескольких переменных <a class=\"anchor\" id=1></a>\n",
    "\n",
    "[к содержанию](#0)\n",
    "\n",
    "Задача оптимизации может быть очень сложной, поскольку функция может иметь десятки, сотни, тысячи или даже миллионы входных данных, а структура функции неизвестна, часто недифференцируема и зашумлена."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Существует функция $n$ переменных $x,y,x,\\dots$, если по некоторому закону каждой системе $n$ чисел ($x,y,x,\\dots$) из некоторого множества ставится в соответствие число $u$.\n",
    "\n",
    "То есть, по сути, относительно функции одной переменной меняется только то, что значение функции зависит от нескольких аргументов, а не от одного:\n",
    "\n",
    "$f(x,y)=x^2y$\n",
    "\n",
    ">**Примечание**. Исключением из этого правила являются так называемые **векторнозначные функции**, для которых значением может быть не одно число, а несколько:\n",
    ">\n",
    ">$f(x)=\\left[\\begin{array}{} \\cos x \\\\ \\sin x \\end{array} \\right]$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, вспомним метод классификации `KNN`. В чём его суть? Ее можно выразить фразой: «Скажи мне, кто твой друг, и я скажу, кто ты». Когда мы получаем обучающую выборку, мы располагаем все её объекты на координатной плоскости (или в пространстве) и затем выбираем количество ближайших соседей. Количество соседей является основным решающим фактором. Обычно это нечётное число. Когда $k=1$, алгоритм называется алгоритмом ближайших соседей. Это самый простой случай.\n",
    "\n",
    "Далее мы получаем новую точку, для которой неизвестен класс и необходимо предсказать метку. Мы отмечаем эту точку на плоскости (или в пространстве) и находим ближайшую к ней точку из обучающей выборки. Мы присваиваем нашей точке метку найденной ближайшей точки.\n",
    "\n",
    "На рисунке ниже точки одного класса обучающей выборки отмечены красными звёздочками, а точки другого — зелёными треугольниками. Мы получаем новую точку (знак вопроса) и видим, что ближе всего к ней расположена красная звёздочка. Значит, нашей новой точке мы присваиваем такой же класс.\n",
    "\n",
    "<img src=m5_img1.png width=400>\n",
    "\n",
    "А что если $k>1$? Предположим, что $P$ — это точка, для которой необходимо предсказать метку. Сначала вы находите $k$ ближайших к $P$ точек, а затем классифицируете точки большинством «голосов» $k$ соседей. Каждый объект «голосует» за свой класс, и класс с наибольшим количеством «голосов» принимается за прогноз. Чтобы найти ближайшие похожие точки, вычисляются  расстояния между точками с использованием мер расстояния, таких как **евклидово расстояние**, **расстояние Хэмминга**, **расстояние Манхэттена** и **расстояние Минковского**. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Итак, алгоритм `KNN` включает в себя следующие основные шаги:\n",
    "\n",
    "* Рассчитать расстояния до всех точек.\n",
    "\n",
    "* Найти ближайших соседей (точки с наименьшим расстоянием).\n",
    "\n",
    "* Посчитать «голоса» за классы.\n",
    "\n",
    "<img src=m5_img2.png>\n",
    "\n",
    "Как уже упоминалось, в этом алгоритме необходимо вычислять расстояние между двумя точками. Для этого используются различные функции нескольких переменных. Рассмотрим самую популярную — **евклидово расстояние**.\n",
    "\n",
    "Если говорить про евклидово расстояние для двух переменных, то его функция записывается как $f(x)=\\sqrt{x^2+y^2}$ и вычисляет корень из суммы квадратов $x$ и $y$. Это не что иное, как расстояние от точки $(x,y)$ до начала координат или длина вектора с координатами $(x,y)$. В качестве области определения для такой функции могут выступать **любые вещественные числа**, а её областью значений являются **все неотрицательные числа**.\n",
    "\n",
    "**Евклидово расстояние до начала координат**:\n",
    "\n",
    "$f(x,y) = \\sqrt{x^2 + y^2} = \\rho (M(x,y), O(0,0)) = \\left\\|\\overrightarrow{r} \\right\\|$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=m5_img3.png>\n",
    "\n",
    "Также евклидово расстояние можно использовать для того, чтобы найти **расстояние между двумя точками**:\n",
    "\n",
    "$f(x,y) = \\sqrt{(x-1)^2 + (y-8)^2} = \\rho (M(x,y), A(1,8))$\n",
    "\n",
    "<img src=m5_img4.png>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Частные производные <a class=\"anchor\" id=2></a>\n",
    "\n",
    "[к содержанию](#0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Безусловные экстремумы <a class=\"anchor\" id=3></a>\n",
    "\n",
    "[к содержанию](#0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Введение в оптимизацию <a class=\"anchor\" id=4></a>\n",
    "\n",
    "[к содержанию](#0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Условные экстремумы. Метод Лагранжа <a class=\"anchor\" id=5></a>\n",
    "\n",
    "[к содержанию](#0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Градиент и антиградиент <a class=\"anchor\" id=6></a>\n",
    "\n",
    "[к содержанию](#0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Градиент и градиентный спуск <a class=\"anchor\" id=7></a>\n",
    "\n",
    "[к содержанию](#0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Практика. Градиентный спуск <a class=\"anchor\" id=8></a>\n",
    "\n",
    "[к содержанию](#0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Итоги <a class=\"anchor\" id=9></a>\n",
    "\n",
    "[к содержанию](#0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
