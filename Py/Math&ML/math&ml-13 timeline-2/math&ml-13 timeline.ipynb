{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## СКОЛЬЗЯЩЕЕ СРЕДНЕЕ\n",
    "\n",
    "В предыдущем модуле мы уже знакомились с экспоненциальным сглаживанием и получением с его помощью прогноза. Помимо простого экспоненциального сглаживания, есть также простое сглаживание, или скользящее среднее.\n",
    "\n",
    "Для получения элементарного случая скользящего среднего проще всего взять среднее арифметическое двух последних наблюдений. Мы получим новый временной ряд, каждый член которого — среднее арифметическое двух соседних значений исходного ряда:\n",
    "\n",
    "$MA_t= \\frac{X_{t-1}+X_t}{2}$\n",
    "\n",
    "Чуть более продвинутый способ — усреднить сразу несколько наблюдений. Это так называемое простое скользящее среднее (`Simple Moving Average`, `SMA`):\n",
    "\n",
    "$SMA_t= \\frac{X_{t-q+1}+...+X_t}{q}$\n",
    "\n",
    "Таким образом, в скользящем среднем мы суммируем несколько последовательных точек временного ряда и делим эту сумму на количество самих точек, то есть считаем математическое усреднение за определённый период."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для сглаживания мы будем использовать встроенный метод [pandas.Series.rolling()](https://pandas.pydata.org/docs/reference/api/pandas.Series.rolling.html) — он принимает на вход параметр window и ожидает после себя агрегирующую функцию для сглаживания (обычно используется среднее). Из преимуществ этого метода можно отметить простоту реализации и интерпретации, из недостатков — чувствительность.\n",
    "\n",
    "Посмотрим на некоторый временной ряд и результаты применения сглаживания к нему:\n",
    "\n",
    "<img src=m13_img1.png width=400>\n",
    "\n",
    "Скользящее среднее с узким окном (размер окна — два дня) неэффективно борется с выбросами.\n",
    "\n",
    "<img src=m13_img2.png width=400>\n",
    "\n",
    "Скользящее среднее с широким окном (размер окна — 30 дней) может привести к потере информации, сгладив полезную информацию.\n",
    "\n",
    "<img src=m13_img3.png width=400>\n",
    "\n",
    "На данном графике с окном размера 15 есть прослеживающийся период и изменение амплитуды с течением времени.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "\n",
    "## ARMA И ARIMA\n",
    "\n",
    "Что получится, если объединить некоторые из изученных нами методов? Уже есть предположения, что такое `ARMA`?\n",
    "\n",
    "Если вы внимательно следили за ходом событий, то смогли догадаться, что `ARMA` — это авторегрессионное скользящее среднее, или модель авторегрессии-скользящего среднего. В ней p авторегрессионных слагаемых и q слагаемых скользящего среднего шумовой компоненты:\n",
    "\n",
    "$X_t = \\alpha + \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + \\dots + \\epsilon_t + \\theta_{t-1}\\epsilon_{t-1} + \\dots + \\theta_{q}\\epsilon_{t-q}$\n",
    "\n",
    "$p$ - параметр авторегрессионной модели\n",
    "\n",
    "$q$ - параметр скользящего среднего\n",
    "\n",
    ">Параметр p мы определяли по графику частичной автокорреляции. Параметр q для скользящего среднего определяют так же, но по коррелограмме (графику автокорреляции)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ARIMA` расшифровывается как `Autoregressive Integrated Moving Average` и включает в себя ещё один параметр ($d$), который означает, что дифференцирование временного ряда порядка $d$ приводит ряд к стационарности и будет подчиняться модели `ARMA`.\n",
    "\n",
    ">$d$ — это тот самый порядок дифференцирования из предыдущего модуля, который приводил нестационарный ряд к стационарности. Это значит, что даже если наш ряд нестационарный, мы можем сделать его стационарным путём взятия разностей. Запомнив получившееся количество дифференцирований, можно смело применять к нему `ARIMA`.\n",
    "\n",
    "И `ARMA`, и `ARIMA` реализованы на `Python` в классе [ARIMA](https://www.statsmodels.org/devel/generated/statsmodels.tsa.arima.model.ARIMA.html) из `statsmodels`. Данному классу необходимо передать в качестве параметров временной ряд и порядок order `(ARIMA(dta, order=(2, 0, 0)))`. Для параметра `order` нужно указать `p`, `d` и `q` (именно в таком порядке), причём для получения `ARMA` необходимо указать `d=0`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Резюмируем:\n",
    "\n",
    "* Если ряд **стационарный**, используем `ARMA`.\n",
    "\n",
    "* Если ряд **нестационарный** (имеет тренд), с помощью дифференцирования определяем порядок d и используем `ARIMA`.\n",
    "Теперь мы можем проверять временные ряды из задач на стационарность и вне зависимости от результата применять к ним одну из статистических моделей."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## SARIMA\n",
    "\n",
    "Модель `ARIMA` отлично учитывает и тренд (благодаря скользящему среднему), и зависимость от предыдущих значений (благодаря авторегрессии), но в ней не хватает учёта сезонности. В таком случае можно добавить к ARIMA учёт сезонности, и тогда мы получим следующую модель — сезонную `ARIMA`, или `SARIMA` (S`easonal ARIMA`).\n",
    "\n",
    "Эта модель очень похожа на ARIMA, за исключением того, что в ней есть дополнительный набор компонентов авторегрессии и скользящего среднего.\n",
    "\n",
    ">`SARIMA` позволяет различать данные по сезонной частоте, а также по их несезонным отличиям. Нахождение лучших для модели параметров можно упростить с помощью средств автоматического поиска, таких как [auto_arima](https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.auto_arima.html) из [pmdarima](http://alkaline-ml.com/pmdarima/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMAX И ARIMAX\n",
    "\n",
    "Вы уже могли заметить, что изученные нами статистические модели совершают предсказания, основываясь только на данных самого временного ряда. Но что если месяц или день недели тоже имеет значение? Или, например, на курс рубля, помимо даты и предыдущих значений, также влияют курсы других валют?\n",
    "\n",
    "Последней статистической моделью, с которой мы познакомимся, будет SARIMAX. Её отличие от предыдущей версии заключается в том, что, помимо данных временного ряда, она учитывает экзогенные переменные. Таким образом мы сможем учитывать не только зависимости внутри данных, но и внешние факторы.\n",
    "\n",
    "Для запуска моделей SARIMA и SARIMAX на Python нужно воспользоваться классом [SARIMAX](https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html). Если вы хотите использовать SARIMA, необходимо задать два обязательных параметра — order и seasonal_order.\n",
    "\n",
    "`order` — это порядок для модели (ARIMA(p, d, q)). В `seasonal_order` необходимо передать ещё четыре параметра:\n",
    "\n",
    "* `P` — сезонный авторегрессионный порядок;\n",
    "* `D` — порядок дифференцирования сезонного ряда;\n",
    "* `Q` — порядок сезонной скользящей средней;\n",
    "* `m` — размер сезонного периода.\n",
    "\n",
    "Если размер сезонного периода m можно определить по сезонной компоненте (мы уже раскладывали ряд на компоненты ранее — [seasonal_decompose()](https://www.statsmodels.org/dev/generated/statsmodels.tsa.seasonal.seasonal_decompose.html)), то остальные параметры удобнее определять автоперебором (мы применим этот способ на практике в следующем юните).\n",
    "\n",
    "Для учёта экзогенных переменных необходимо передать в класс SARIMAX параметр exog=x. В x должны находиться другие временные ряды, например курс доллара (x), который может влиять на курс рубля (y), или пометка, является ли каждая из дат праздничным днём.\n",
    "\n",
    "Хороший пример реализации SARIMAХ приведён в [официальной документации](https://www.statsmodels.org/stable/examples/notebooks/generated/statespace_sarimax_faq.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Обратите внимание, что также существует модель `ARIMAX`. Уже есть предположения, когда использовать эту модель? В отличие от SARIMAX, ARIMAX не учитывает сезонную составляющую, но имеет все преимущества ARIMA и учитывает экзогенные переменные."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## КАК СРАВНИВАТЬ ЭТИ МОДЕЛИ?\n",
    "\n",
    "Одним из распространённых способов является сравнение качества моделей по **критерию Акаике** (`AIC`). Этот информационный критерий вознаграждает модель за качество приближения обученного временного ряда к фактическому, а также «штрафует» её за использование излишнего количества параметров. Принято считать, что модель с наименьшим значением критерия `AIC` является наилучшей.\n",
    "\n",
    "Для оценки модели критерием AIC необязательно пользоваться дополнительными методами. Этот критерий, как и другая информация, отображается после обучения модели при вызове встроенного метода `fit_model.summary()`.\n",
    "\n",
    "## КАК ВЫБРАТЬ ПОДХОДЯЩУЮ МОДЕЛЬ?\n",
    "\n",
    "<img src=m13_img4.png>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ДОПОЛНИТЕЛЬНО:\n",
    "\n",
    "* Оценка качества модели информационными критериями продемонстрирована в [этой статье](https://www.projectpro.io/article/how-to-build-arima-model-in-python/544).\n",
    "\n",
    "* Ещё один пример с расчётом критерия для различных комбинаций параметров представлен [здесь](https://www.8host.com/blog/prognozirovanie-vremennyx-ryadov-s-pomoshhyu-arima-v-python-3/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Интерполяция и сэмплирование. Практика\n",
    "\n",
    "1. Проверьте данные на наличие пропущенных дат. Помимо визуального способа, это можно сделать с помощью метода [DataFrame.asfreq()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.asfreq.html), передав в него параметр частоты, например `‘d’` — день, `‘m’` — месяц. Все алиасы для параметров частоты доступны [по ссылке](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#timeseries-offset-aliases).\n",
    "Подсказка\n",
    "```Python\n",
    "\n",
    "    df.asfreq(freq='AS')\n",
    "```\n",
    "2. Проверьте данные на наличие пропущенных значений (`Series.isna().sum()`).\n",
    "\n",
    "3. Обратите внимание, что иногда пропуски в данных могут быть заполнены нулями. Сделайте проверку на наличие нулевых значений и в случае нахождения замените их на NaN. Воспользуйтесь кодом ниже:\n",
    "``` Python\n",
    "    import numpy as np\n",
    "    df['GDP (current US$)'] = df['GDP (current US$)'].apply(lambda x: np.nan if x==0 else x)\n",
    "```\n",
    "4. Для заполнения пропусков выполните интерполяцию с использованием метода .interpolate().\n",
    "Подсказка\n",
    "``` Python\n",
    "    df['GDP (current US$)'].interpolate(method='linear')\n",
    "```\n",
    "5. Проверьте полученный ряд на стационарность, определите параметры модели (`ARIMA/ARMA`) и запустите модель.\n",
    "\n",
    "6. Изменился ли `AIC` критерий построенной модели по сравнению с моделью на неинтерполированных данных? Сделайте вывод."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Файл `project-Part2.ipynb`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модели прогнозирования гетероскедастичности\n",
    "\n",
    ">**Дисперсия** — это статистический показатель, показывающий меру разброса данных вокруг среднего значения.\n",
    "\n",
    "Если мы проведём на графике усреднённую прямую (красный пунктир), будет заметно, что ближе к 1997 году точки имеют больший разброс, чем в начале, то есть дисперсия данных изменчива во времени.\n",
    "\n",
    "Подобная неоднородность наблюдений, выражающаяся в неодинаковой дисперсии, называется гетероскедастичностью.\n",
    "\n",
    "Такой характер дисперсии можно часто встретить во временных рядах из финансовой сферы, поскольку они более подвержены гетероскедастичности.\n",
    "\n",
    "Отсутствие гетероскедастичности называется гомоскедастичностью.\n",
    "\n",
    "Сравните графики временных рядов ниже:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Гомоскедастичный временной ряд | Гетероскедастичный временной ряд |\n",
    "| - | - |\n",
    "| <img src=m13_img5.png width=400> | <img src=m13_img6.png width=400> |\n",
    "| <img src=m13_img7.png width=400> | <img src=m13_img8.png width=400> |\n",
    "| <img src=m13_img9.png width=400> | <img src=m13_img10.png width=400> |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изученные нами модели помогают учесть тренд и сезонность. **Но что насчёт дисперсии?** Обычно, если временной ряд обладает гетероскедастичностью, такие модели не способны уловить изменение дисперсии во времени, и их предсказания выглядят примерно так:\n",
    "\n",
    "<img src=m13_img11.png width=800>\n",
    "\n",
    "Видно, что в предсказании мы не учли разброс — прогноз хочется «вытянуть» вверх, таким образом увеличивая дисперсию. Такая проблема часто возникает в волатильных временных рядах. Понятие волатильности также относится к финансовым временным рядам и является одним из самых важных финансовых показателей и понятий в управлении финансовыми рисками.\n",
    "\n",
    ">**Волатильность** представляет собой меру риска использования финансового инструмента за заданный промежуток времени. Иными словами, волатильность показывает меру изменчивости и часто измеряется в процентах или долях. Предсказание волатильности, например, позволяет инвесторам определить риск приобретения финансового инструмента.\n",
    "\n",
    "Для таких данных с непостоянной дисперсией был разработан ещё один класс моделей — семейство моделей авторегрессионной условной гетероскедастичности, или `ARCH` (`Autoregressive Conditional Heteroscedastic Model`)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARCH И GARCH\n",
    "\n",
    "Давайте сначала разберёмся, как устроена модель `ARCH`, а затем поговорим о том, как правильно её применять.\n",
    "\n",
    "Ниже представлен график ошибок предсказания некоторого временного ряда. Что с ним не так?\n",
    "\n",
    "<img src=m13_img12.png>\n",
    "\n",
    "В прошлом модуле мы обсуждали, что в хорошей модели, учитывающей все зависимости, в остатках должен получиться белый шум. Однако белый шум должен иметь одинаковое распределение на всех участках временного ряда, а значит, иметь одинаковые математическое ожидание и дисперсию.\n",
    "\n",
    "Если на графике выше с математическим ожиданием всё в порядке (среднее значение в нуле), то к дисперсии есть вопросы, так как видно, что она различна на разных промежутках (разброс в разные годы отличен).\n",
    "\n",
    "$\\sigma^2(t)=a+\\sum_{i=1}^{q}b_ir_{t-1}^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Здесь $q$ — количество слагаемых, которые влияют на текущее значение, а $b$ — весовые коэффициенты, которые влияют на степень значимости предыдущих изменений дисперсии ($r$). То есть волатильность моделируется в виде суммы константы ($a$ — базовая волатильность, константа) и линейной функции абсолютных значений изменения нескольких последних цен.\n",
    "\n",
    ">**Обратите внимание**: из-за квадрата в формуле волатильность предсказывается только в абсолютном значении (то есть по модулю).\n",
    "\n",
    "Чуть позднее другой учёный, Тим Боллерслерв, предложил обобщённую концепцию модели `ARCH` — `GARCH` (`Generalized Autoregressive Conditional Heteroscedastic Model`). Модель предполагала, что на изменчивость дисперсии влияют не только предыдущие изменения показателей, но и предыдущие оценки дисперсии (значение дисперсии).\n",
    "\n",
    "$\\sigma^2(t)=a+\\sum_{i=1}^{q}b_ir_{t-1}^2+\\sum_{i=1}^{p}c_i \\sigma_{t-1}^2$\n",
    "\n",
    "* первая часть формулы — ARCH-модель;\n",
    "\n",
    "* $p$ — количество оценок, предшествующих текущей, которые влияют на текущее значение;\n",
    "\n",
    "* $c$ — это весовые коэффициенты, которые влияют на степень значимости предыдущих дисперсий ($\\sigma^2$).\n",
    "Как и в случае AR-моделей, эти коэффициенты настраиваются автоматически в процессе обучения."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ARCH и GARCH](https://arch.readthedocs.io/en/latest/univariate/introduction.html) — наиболее популярные модели, используемые при прогнозе волатильности. Существуют и их модернизации, такие как A-GARCH, E-GARCH и многие другие. В будущем, если вам придётся тесно работать с финансовыми временными рядами, вы сможете самостоятельно познакомиться с этими моделями.\n",
    "\n",
    "## КОГДА ПРИМЕНЯТЬ ARCH И GARCH?\n",
    "\n",
    "* Когда ряд похож на белый шум, но при этом в нём **присутствует гетероскедастичность**.\n",
    "Чтобы определить, является ли ряд гетероскедастичным (с меняющейся дисперсией), можно отобразить его квадраты на графике и понаблюдать за поведением дисперсии.\n",
    "\n",
    "* Когда после применения AR-модели **остатки (ошибки модели) тоже являются гетероскедастичными**. В этом случае вы также можете прогнозировать дисперсию ошибок и использовать её в итоговом предсказании. Для этого необходимо суммировать результаты AR-модели с результатами ARCH.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=m13_img13.png>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ДОПОЛНИТЕЛЬНО:\n",
    "\n",
    "* [Задача](https://medium.com/@ranjithkumar.rocking/time-series-model-s-arch-and-garch-2781a982b448) моделирования остатков.\n",
    "\n",
    "* [Короткая лекция](https://medium.com/@ranjithkumar.rocking/time-series-model-s-arch-and-garch-2781a982b448) с разбором математики ARCH и GARCH."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Валидация временных рядов"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последовательное разбиение может выглядеть так (`train`, `test`):\n",
    "\n",
    "<img src=m13_img14.png>\n",
    "\n",
    "То есть в случае с данными на графике выше правильно будет разбить их как `train = [0,1,2,3,4,5]` и `test = [6,7,8,9]`. Соотношение объёма выборок, конечно, может быть другим, но главное — выборки должны идти последовательно.\n",
    "\n",
    "## АНАЛОГ КРОСС-ВАЛИДАЦИИ ДЛЯ ВРЕМЕННЫХ РЯДОВ\n",
    "\n",
    "Если использовать обычную кросс-валидацию, информация из будущего снова просочится в алгоритм. Поэтому вместо кросс-валидации используются:\n",
    "\n",
    "* **walk forward validation**;\n",
    "\n",
    "* **множественное разбиение**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=m13_img15.png width=800>\n",
    "\n",
    ">Обратите внимание на изображения выше. В обоих случаях мы также соблюдаем последовательность в разбиении — сравните с обычной кросс-валидацией (слева). Оба разбиения действуют по схожему принципу — делят выборки упорядоченно, чтобы подвыборка test всегда выбиралась после подвыборок train. Отличием будет то, что в walk forward validation размер обучающих выборок во всех фолдах (разбиениях) будет одинаковым, а во множественном разбиении в каждом новом фолде данные обучающей выборки накапливаются."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Разбейте временной ряд на три набора (`3 train + 3 test`). Для этого воспользуйтесь классом [TimeSeriesSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html) из `sklearn.model_selection` с параметрами `n_splits=3` и `test_size=7` (или `max_train_size=df.sales.count()-7`). Наборы данных можно получить, вызвав метод `split` (временной ряд) у инициализированного `TimeSeriesSplit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object TimeSeriesSplit.split at 0x000001CAD0ECBCF0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "df = pd.read_csv(\"sp500.zip\", parse_dates=[\"Date\"], index_col=[\"Date\"])\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3, test_size=7)\n",
    "\n",
    "train_test_groups = tscv.split(df)\n",
    "train_test_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN size: 6671 TEST size: 7\n",
      "TRAIN size: 6678 TEST size: 7\n",
      "TRAIN size: 6685 TEST size: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                    spx\n",
       " Date                   \n",
       " 1994-01-06   467.119995\n",
       " 1994-01-07   469.899994\n",
       " 1994-01-10   475.269989\n",
       " 1994-01-11   474.130005\n",
       " 1994-01-12   474.170013\n",
       " ...                 ...\n",
       " 2019-08-15  2847.600098\n",
       " 2019-08-16  2888.679932\n",
       " 2019-08-19  2923.649902\n",
       " 2019-08-20  2900.510010\n",
       " 2019-08-21  2924.429932\n",
       " \n",
       " [6685 rows x 1 columns],\n",
       "                     spx\n",
       " Date                   \n",
       " 2019-08-22  2922.949951\n",
       " 2019-08-23  2847.110107\n",
       " 2019-08-26  2878.379883\n",
       " 2019-08-27  2869.159912\n",
       " 2019-08-28  2887.939941\n",
       " 2019-08-29  2924.580078\n",
       " 2019-08-30  2926.459961)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_groups = tscv.split(df)\n",
    "i = 0\n",
    "train = []\n",
    "test = []\n",
    "for train_index, test_index in train_test_groups:\n",
    "    print(\"TRAIN size:\", len(train_index), \"TEST size:\", len(test_index))\n",
    "    train.append(df.iloc[train_index])\n",
    "    test.append(df.iloc[test_index])\n",
    "    \n",
    "train[-1], test[-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Файл `project-Part3.ipynb`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В скринкасте мы получаем предсказания с помощью `model_results.forecast()`.\n",
    "\n",
    "Любое обращение к методу `forecast()` возвращает объект `ARCHModelForecast` с тремя основными атрибутами:\n",
    "\n",
    "* `.mean` — условное среднее значение ряда;\n",
    "\n",
    "* `.variance` — условная дисперсия прогноза;\n",
    "\n",
    "* `.residual_variance` — прогнозируемая условная дисперсия остатков. `Residual_variance` будет отличаться от `variance` в случаях, когда значения ряда (не дисперсии, а именно сами значения) имеют зависимость от себя в прошлом. В скринкасте для предсказания дисперсии мы используем `residual_variance`, так как в нашем случае ряд стационарен и `residual_variance` и variance возвращают одинаковые значения."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Другие методы предсказания временных рядов\n",
    "\n",
    "## PROPHET\n",
    "\n",
    ">`Prophet` — это метод прогнозирования данных временных рядов на основе AR-модели, в которой учтены годовая, еженедельная и ежедневная сезонности, а также эффекты праздничных дней.\n",
    "\n",
    "Prophet лучше всего работает с временными рядами, которые имеют сильные сезонные эффекты, а данные накоплены за несколько сезонов. Алгоритм устойчив к отсутствующим данным и сдвигам в тренде и обычно хорошо справляется с выбросами.\n",
    "\n",
    "[Prophet](https://facebook.github.io/prophet/docs/quick_start.html#python-api) - библиотека с открытым исходным кодом, выпущенная командой Facebook Core Data Science. Для загрузки метод также доступен в [PyPI](https://pypi.org/project/prophet/) (через pip install).\n",
    "\n",
    "Prophet следует `API` модели `sklearn`. Это значит, что мы можем пользоваться им и его методами так же, как и в случае с моделями из `sklearn`: для инициализации модели создаётся экземпляр класса Prophet (`myModel = Prophet()`), а затем вызываются его методы обучения (`.fit()`) и прогнозирования (`.predict()`). Входные данные для методов Prophet должны представлять собой датафрейм с двумя столбцами — `DS` и `Y`.\n",
    "\n",
    "Столбец `DS` (отметка даты) должен иметь временной формат (`DateTime`), например ГГГГ-ММ-ДД — для даты или ГГГГ-ММ-ДД ЧЧ:ММ:СС — для отметки времени.\n",
    "Столбец `Y` должен быть числовым и представлять измерение, которое мы хотим спрогнозировать."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEURALPROPHET\n",
    "\n",
    ">[NeuralProphet](https://neuralprophet.com/) — основанная на [PyTorch](https://pytorch.org/) усовершенствованная и более сложная модель, которая комбинирует в себе преимущества традиционных моделей для анализа временных рядов и методов глубокого обучения. `NeuralProphet` можно установить с помощью pip (`pip install neuralprophet`).\n",
    "\n",
    "Вы ещё не знакомы с нейронными сетями, поэтому мы не будем углубляться в подробности реализации данного алгоритма. Тем не менее вы можете смело использовать эту библиотеку, так как запустить её в несколько шагов «из коробки» не представляет большого труда.\n",
    "\n",
    ">Познакомиться и поэкспериментировать с NeuralProphet вы можете, разбирая примеры из официального репозитория проекта на GitHub. Например, обратите внимание на решение [задачи](https://github.com/ourownstory/neural_prophet/blob/main/tutorials/application-example/energy_hospital_load.ipynb) прогнозирования энергетической нагрузки на больницу в Сан-Франциско.\n",
    "\n",
    "В [этой русскоязычной статье](https://habr.com/ru/company/otus/blog/555700/) представлен более подробный разбор алгоритма на примере решения задачи предсказания ежедневных просмотров страниц на Wikipedia, но уже с помощью `NeuralProphet`.\n",
    "\n",
    "## ДОПОЛНИТЕЛЬНО:\n",
    "\n",
    "Если вас заинтересовали библиотеки `Prophet` и `NeuralProphet`, рекомендуем обратить внимание [на эту статью](https://towardsdatascience.com/prophet-vs-neuralprophet-fc717ab7a9d8). Помимо математического описания, там сравниваются результаты работы этих алгоритмов на реальной задаче."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
