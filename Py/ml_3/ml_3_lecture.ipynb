{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-3 Обучение с учителем. Классификация\n",
    "###  Содержание <a class=\"anchor\" id=0></a>\n",
    "- [1. Введение](#1)\n",
    "- [2. Логическая регрессия](#2)\n",
    "- [2.2 Поиск параметров логической регрессии](#2-2)\n",
    "- [3. Метрики классификации. Мультиклассовая классификация](#3)\n",
    "- [4. Логическия регрессия. Практика](#4)\n",
    "- [5. Деревья решений](#5)\n",
    "- [6. Введение в ансамбли: бэггинг. Случайные лес](#6)\n",
    "- [7. Деревья решений и случайный лес. Практика](#7)\n",
    "- [8. Итоги](#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Введение <a class=\"anchor\" id=1></a>\n",
    "\n",
    "[к содержанию](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Логическая регрессия <a class=\"anchor\" id=2></a>\n",
    "\n",
    "[к содержанию](#0)\n",
    "\n",
    "Что вообще означает `«решить задачу классификации»`? Это значит **построить разделяющую поверхность в пространстве признаков**, которая делит пространство на части, каждая из которых соответствует определённому классу. \n",
    "\n",
    "Ниже представлены примеры разделяющих поверхностей, которые производят бинарную классификацию. Красным и синим цветом обозначены классы, зелёным — собственно поверхность, которая делит пространство признаков на две части. В каждой из этих частей находятся только наблюдения определённого класса.\n",
    "\n",
    "<img src=ml3_img1.png>\n",
    "\n",
    "Модели, которые решают задачу классификации, называются классификаторами (`classifier`).\n",
    "\n",
    "Если взять в качестве разделяющей поверхности некоторую плоскость (ровная поверхность на первом рисунке), то мы получаем модель логистической регрессии, которая тесно связана с рассмотренной нами ранее линейной регрессией.\n",
    "\n",
    "Давайте для начала вспомним, как выглядит уравнение модели линейной регрессии в общем случае:\n",
    "\n",
    "<img src=ml3_img2.png>\n",
    "\n",
    ">Но всё это работает только в том случае, когда целевой признак `y`, который мы хотим предсказать, является числовым, например цена, вес, время аренды и т. д.\n",
    "\n",
    "Что же делать с этой моделью, когда целевой признак `y` является категориальным? Например, является письмо спамом или обычным письмом?\n",
    "\n",
    "Можно предположить, что, раз у нас есть две категории, мы можем обозначить категории за `y = 1` (Спам) и `y = 0` (Не спам) и обучить линейную регрессию предсказывать `0` и `1`.\n",
    "\n",
    "Но результат будет очень плохим. Выглядеть это будет примерно так:\n",
    "\n",
    "<img src=ml3_img3.png>\n",
    "\n",
    "Для больших значений `x` прямая будет выдавать значения **больше 1**, а для очень маленьких — **меньше 0**. Что это значит? Непонятно. Непонятно и то, что делать со значениями в диапазоне от 0 до 1. Да, можно относить значения на прямой выше 0.5 к классу 1, а меньше либо равным 0.5 — к классу 0, но это всё «костыли».\n",
    "\n",
    ">Идея! Давайте переведём задачу классификации в задачу регрессии. Вместо предсказания класса будем предсказывать вероятность принадлежности к этому классу. \n",
    "\n",
    "Модель должна выдавать некоторую вероятность `P`, которая будет определять, принадлежит ли данный объект к классу 1: например, вероятность того, что письмо является спамом. При этом вероятность того, что письмо является обычным письмом (класс 0), определяется как `Q = 1 - P`.  \n",
    "\n",
    "Когда модель будет обучена на предсказание вероятности, мы зададим некоторый порог вероятности. Если предсказанная вероятность будет выше этого порога, мы определим объект к классу 1, а если ниже — к классу 0.\n",
    "\n",
    "Например, стандартный порог равен `0.5`. То есть если вероятность `P > 0.5`, мы будем считать письмо спамом, а если `P <= 0.5` — обычным информативным письмом.\n",
    "\n",
    "В итоге мы добьёмся того, что будем предсказывать не дискретный категориальный, а непрерывный числовой признак, который лежит в диапазоне `[0, 1]`. А это уже знакомая нам задача регрессии.\n",
    "\n",
    ">Однако остался главный вопрос: как научить модель предсказывать вероятности, ведь они должны лежать строго в диапазоне от 0 до 1, а предсказания линейной регрессии лежат в диапазоне от `-inf` до `+inf`? \n",
    "\n",
    "Тут-то мы и приходим к модели логистической регрессии — **регрессии вероятностей**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ОБЩЕЕ ПРЕДСТАВЛЕНИЕ О ЛОГИСТИЧЕСКОЙ РЕГРЕССИИ\n",
    "\n",
    "Логистическая регрессия (`Logistic Regression`) — одна из простейших моделей для решения задачи классификации. Несмотря на простоту, модель входит в топ часто используемых алгоритмов классификации в `Data Science`.\n",
    "\n",
    "В основе логистической регрессии лежит логистическая функция (`logistic function`) `sigma` — отсюда и название модели. Однако более распространённое название этой функции — сигмόида (`sigmoid`). Записывается она следующим образом:\n",
    "\n",
    "<img src=ml3_img4.png>\n",
    "\n",
    "<img src=ml3_img5.png>\n",
    "\n",
    "<img src=ml3_img6.png>\n",
    "\n",
    "## Чего мы добились таким преобразованием?\n",
    "\n",
    "Если мы обучим модель, то есть подберём  коэффициенты `w0, w1,... , w_m` (как их найти, обсудим чуть позже) таким образом, что для объектов класса 1 модель линейной регрессии начнёт выдавать положительное число, а для класса 0 — выдавать отрицательное число, то тогда, подставив предсказание линейной регрессии `z` в сигмоиду, мы сможем получать вероятности принадлежности к каждому из классов в диапазоне от `0` до `1`.\n",
    "\n",
    "Далее по порогу вероятности мы сможем определять, к какому классу принадлежит объект.\n",
    "\n",
    "Это и есть наша цель. Мы свели задачу классификации к задаче регрессии для предсказания вероятностей. \n",
    "\n",
    "Для бинарной классификации описанное выше будет выглядеть следующим образом:\n",
    "\n",
    "<img src=ml3_img7.png>\n",
    "\n",
    "<img src=ml3_img8.png>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Разберёмся с геометрией\n",
    "\n",
    "Возьмём частный случай, когда класс объекта зависит от двух признаков — `x1` и `x2`.\n",
    "\n",
    "## Рассмотрим пример.\n",
    "\n",
    "Мы пытаемся предсказать поступление студента в университет в зависимости от результатов двух экзаменов. Целевой признак  — результат поступления в аспирантуру (admission outcome) с двумя возможными значениями: поступил или не поступил. Факторы: `x1` — результат сдачи первого экзамена (Exam1 Score) и `x2` — результат сдачи второго (Exam 2 Score). Будем предсказывать вероятность поступления с помощью логистической регрессии.\n",
    "\n",
    "Изобразим зависимость в пространстве двух факторов (вид сверху) в виде диаграммы рассеяния, а целевой признак отобразим в виде точек (непоступившие) и крестиков (поступившие).\n",
    "\n",
    "Если рассматривать уравнение линейной регрессии отдельно от сигмоиды, то геометрически построить логистическую регрессию на основе двух факторов — значит найти такие коэффициенты `w0`, `w1` и `w2` уравнения плоскости, при которых наблюдается наилучшее разделение пространства на две части.\n",
    "\n",
    "`z = w0 + w1*x1 + w2*x2`\n",
    "\n",
    "Тогда выражение для `z` будет задавать в таком пространстве плоскость (в проекции вида сверху — прямую), которая разделяет всё пространство на две части. Над прямой вероятность поступления будет `>0.5`, а под прямой `<0.5`:\n",
    "\n",
    "<img src=ml3_img9.png>\n",
    "\n",
    "## В чём математический секрет?\n",
    "\n",
    "Математически подстановка в уравнение плоскости точки, которая не принадлежит ей (находится ниже или выше), означает вычисление расстояния от этой точки до плоскости.\n",
    "\n",
    "* Если точка находится ниже плоскости, расстояние будет отрицательным `z < 0`.\n",
    "* Если точка находится выше плоскости, расстояние будет положительным `z > 0`.\n",
    "* Если точка находится на самой плоскости, `z = 0`.\n",
    "\n",
    "Мы знаем, что подстановка отрицательных чисел в сигмоиду приведёт к вероятности `P < 0.5`, а постановка положительных — к вероятности `P > 0.5`. \n",
    "\n",
    ">Таким образом, ключевым моментом в предсказании логистической регрессии является расстояние от точки до разделяющей плоскости в пространстве факторов. Это расстояние в литературе часто называется **отступом** (`margin`). \n",
    "\n",
    "В этом и состоит секрет работы логистической регрессии.\n",
    "\n",
    ">Чем больше расстояние от точки, находящейся выше разделяющей плоскости, до самой плоскости, тем больше оценка вероятности принадлежности к классу 1.\n",
    "\n",
    "Попробуйте подставить различные координаты точек в модель логистической регрессии и убедитесь в этом.\n",
    "\n",
    "Можно построить тепловую карту, которая показывает, чему равны вероятности в каждой точке пространства:\n",
    "\n",
    "<img src=ml3_img10.png>\n",
    "\n",
    ">На рисунке точки, которые относятся к классу непоступивших абитуриентов, лежащие ниже разделяющей плоскости, находятся в красной зоне. Чем насыщеннее красный цвет, тем ниже вероятность того, что абитуриент поступит в аспирантуру. И наоборот, точки, которые относятся к классу поступивших абитуриентов, лежащие выше разделяющей плоскости, находятся в синей зоне. Чем насыщеннее синий цвет, тем выше вероятность того, что абитуриент поступит в аспирантуру.\n",
    "\n",
    "<img src=ml3_img11.png>\n",
    "\n",
    "<img src=ml3_img12.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ПОИСК ПАРАМЕТРОВ ЛОГИСТИЧЕСКОЙ РЕГРЕССИИ <a class=\"anchor\" id=2-2></a>\n",
    "\n",
    "[к содержанию](#0)\n",
    "\n",
    "Итак, мы разобрались с тем, как выглядит модель логистической регрессии и что она означает в геометрическом смысле.\n",
    "\n",
    ">Но остался один главный вопрос: как найти такие коэффициенты `w = (w0, w1,... ,w_m)` , чтобы гиперплоскость разделяла пространство наилучшим образом?\n",
    "\n",
    "Вновь обратимся к нашей схеме минимизации эмпирического риска:\n",
    "\n",
    "<img src=ml3_img13.png width=800>\n",
    "\n",
    "Можно предположить, что стоит использовать метод наименьших квадратов. Введём функцию ошибки — средний квадрат разности `MSE` между истинными классами `y` и предсказанными классами `y_` и попытаемся его минимизировать.\n",
    "\n",
    "Сразу можно достоверно предсказать, что результат такого решения будет плохим, поэтому воздержимся от его использования.\n",
    "\n",
    "Здесь нужен другой подход. Это **метод максимального правдоподобия** (`Maximum Likelihood Estimation — MLE`). \n",
    "\n",
    ">**Правдоподобие** — это оценка того, насколько вероятно получить истинное значение целевой переменной `y` при данных `x` и параметрах `w`. \n",
    "\n",
    "Данный метод позволяет получить функцию правдоподобия.\n",
    "\n",
    ">Цель метода — найти такие параметры `w = (w0, w1,... ,w_m)`, в которых наблюдается максимум функции правдоподобия. Подробнее о выводе формулы вы можете прочитать здесь.\n",
    "\n",
    "А мы пока что опустим математические детали метода и приведём только конечную формулу:\n",
    "\n",
    "<img src=ml3_img14.png>\n",
    "<img src=ml3_img15.png>\n",
    "\n",
    "**Примечание**. К сожалению, функция `likelihood` не имеет интерпретации, то есть нельзя сказать, что значит число `2.34` в контексте правдоподобия.\n",
    "\n",
    "## Цель — найти такие параметры, при которых наблюдается максимум этой функции.\n",
    "\n",
    "Теперь пора снова применить магию математики, чтобы привести задачу к привычному нам формату минимизации эмпирического риска. По правилам оптимизации, если поставить перед функцией минус, то задача оптимизации меняется на противоположную: был поиск максимума — станет поиском минимума.\n",
    "\n",
    "Таким образом мы получим функцию потерь `L(w)`, которая носит название «**функция логистических потерь**», или `logloss`. Также часто можно встретить название кросс-энтропия, или `cross-entropy` `loss`:\n",
    "\n",
    "<img src=ml3_img16.png>\n",
    "\n",
    "Математическую реализацию вычисления градиента для `logloss` мы обсудим далее в курсе, а пока нас интересует исключительно его смысл."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы уже знаем, что для того, чтобы повысить шанс пройти мимо локальных минимумов функции потерь, используется не сам градиентный спуск, а его модификации: например, можно использовать уже знакомый нам стохастический градиентный спуск (`SGD`).\n",
    "\n",
    "Помним, что применение градиентного спуска требует предварительного масштабирования данных (стандартизации/нормализации). В реализации логистической регрессии в `sklearn` предусмотрено ещё несколько методов оптимизации, для которых масштабирование не обязательно. О них мы упомянём в практической части модуля.\n",
    "\n",
    "Во избежание переобучения модели в функцию потерь логистической регрессии традиционно добавляется **регуляризация**. В реализации логистической регрессии в `sklearn` она немного отличается от той, что мы видели ранее для линейной регрессии.\n",
    "\n",
    "<img src=ml3_img17.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# В этом месте можно полистать ноутбук `Classification.LogisticRegression.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Метрики классификации. Мультиклассовая классификация <a class=\"anchor\" id=3></a>\n",
    "\n",
    "[к содержанию](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Логическия регрессия. Практика <a class=\"anchor\" id=4></a>\n",
    "\n",
    "[к содержанию](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Деревья решений <a class=\"anchor\" id=5></a>\n",
    "\n",
    "[к содержанию](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Введение в ансамбли: бэггинг. Случайные лес <a class=\"anchor\" id=6></a>\n",
    "\n",
    "[к содержанию](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Деревья решений и случайный лес. Практика <a class=\"anchor\" id=7></a>\n",
    "\n",
    "[к содержанию](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Итоги <a class=\"anchor\" id=8></a>\n",
    "\n",
    "[к содержанию](#0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c3380a37b4678e1f5e651331348d62bc6038aef0d5f414da260f404a34792558"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
