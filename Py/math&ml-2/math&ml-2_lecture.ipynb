{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATH&ML-2 Линейная алгебра в констекте линейных методов. Часть2\n",
    "###  Содержание <a class=\"anchor\" id=0></a>\n",
    "\n",
    "- [2. Неоднородные СЛАУ](#2)\n",
    "- [3. Линейная регрессия МНК](#3)\n",
    "- [4. Стандартизация векторов и матрица корреляции](#4)\n",
    "- [5. Практика. Лин.регрессия МНК](#5)\n",
    "- [6. Полиноминальная регрессия](#6)\n",
    "- [7. Регуляция](#7)\n",
    "- [8. Практика. Полиноминальная регрессия и регуляция](#8)\n",
    "- [9. Итоги](#9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Неоднородные СЛАУ <a class=\"anchor\" id=2></a>\n",
    "\n",
    "[к содержанию](#0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Совокупность уравнений первой степени, в которых каждая переменная и коэффициенты в ней являются вещественными числами, называется **системой линейных алгебраических уравнений** (`СЛАУ`) и в общем случае записывается как:\n",
    ">\n",
    ">$\\left\\{ \\begin{array}{c} a_{11}x_1+a_{12}x_2+\\dots +a_{1m}x_m=b_1 \\\\ a_{21}x_1+a_{22}x_2+\\dots +a_{2m}x_m=b_2 \\\\ \\dots \\\\ a_{n1}x_1+a_{n2}x_2+\\dots +a_{nm}x_m=b_n \\end{array} \\right.\\ (1),$\n",
    ">\n",
    ">где\n",
    ">\n",
    ">* $n$— количество уравнений;\n",
    ">\n",
    ">* $m$ — количество переменных;\n",
    ">\n",
    ">* $x_i$ — неизвестные переменные системы;\n",
    ">\n",
    ">* $a_{ij}$ — коэффициенты системы;\n",
    ">\n",
    ">* $b_i$ — свободные члены системы.\n",
    ">\n",
    ">\n",
    ">\n",
    ">СЛАУ (1) называется **однородной**, если все свободные члены системы равны 0 $b_1=b_2=⋯=b_n=0$:\n",
    ">\n",
    ">$\\textrm{С}\\textrm{Л}\\textrm{А}\\textrm{У}-\\textrm{о}\\textrm{д}\\textrm{н}\\textrm{о}\\textrm{р}\\textrm{о}\\textrm{д}\\textrm{н}\\textrm{а}\\textrm{я},\\ \\textrm{е}\\textrm{с}\\textrm{л}\\textrm{и}\\ \\forall b_i=0$\n",
    ">\n",
    ">\n",
    ">\n",
    ">СЛАУ (1) называется **неоднородной**, если хотя бы один из свободных членов системы отличен от 0:\n",
    ">\n",
    ">$\\textrm{С}\\textrm{Л}\\textrm{А}\\textrm{У}- \\textrm{н}\\textrm{е}\\textrm{о}\\textrm{д}\\textrm{н}\\textrm{о}\\textrm{р}\\textrm{о}\\textrm{д}\\textrm{н}\\textrm{а}\\textrm{я},\\ \\textrm{е}\\textrm{с}\\textrm{л}\\textrm{и}\\ \\exists b_i\\neq 0$\n",
    ">\n",
    ">\n",
    ">\n",
    ">**Решением** СЛАУ (1) называется такой набор значений неизвестных переменных $x_1,x_2,…,x_n$ при котором каждое уравнение системы превращается в равенство.\n",
    ">\n",
    ">\n",
    ">\n",
    ">СЛАУ (1) называется **определённой**, если она имеет только одно решение, и **неопределённой**, если возможно больше одного решения."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним, что СЛАУ можно записать в матричном виде:\n",
    "\n",
    "$A\\overrightarrow{x}=\\overrightarrow{b}$\n",
    "\n",
    "$\\left( \\begin{array}{cccc} a_{11} & a_{12} & \\dots & a_{1m} \\\\ a_{21} & a_{22} & \\dots & a_{2m} \\\\ \\dots & \\dots & \\dots & \\dots \\\\ a_{n1} & a_{n2} & \\dots & a_{nm} \\end{array} \\right) \\cdot \\left( \\begin{array}{c} x_1 \\\\ x_2 \\\\ \\dots \\\\ x_m \\end{array} \\right)=\\left( \\begin{array}{c} b_1 \\\\ b_2 \\\\ \\dots \\\\ b_n \\end{array} \\right)$\n",
    "\n",
    "где $A$ — матрица системы, $\\overrightarrow{x}$ — вектор неизвестных коэффициентов, а $b$ — вектор свободных коэффициентов. \n",
    "\n",
    "Давайте введём новое для нас определение.\n",
    "\n",
    ">**Расширенной матрицей системы $(A|b)$ неоднородных СЛАУ** называется матрица, составленная из исходной матрицы и вектора свободных коэффициентов (записывается через вертикальную черту):\n",
    ">\n",
    ">$(A \\mid \\vec{b})=\\left(\\begin{array}{cccc|c} a_{11} & a_{12} & \\ldots & a_{1 m} & b_{1} \\\\ a_{21} & a_{22} & \\ldots & a_{2 m} & b_{2} \\\\ \\ldots & \\ldots & \\ldots & \\ldots & \\ldots \\\\ a_{n 1} & a_{n 2} & \\ldots & a_{n m} & b_{n} \\end{array}\\right)$\n",
    "\n",
    "Расширенная матрица системы — это обычная матрица. Черта, отделяющая коэффициенты $a_{ij}$ от свободных членов $b_i$ — чисто символическая. \n",
    "\n",
    "Над расширенной матрицей неоднородной СЛАУ можно производить те же самые действия, что и над обычной, а именно:\n",
    "\n",
    "* складывать/вычитать между собой строки/столбцы матрицы;\n",
    "* умножать строки/столбцы на константу;\n",
    "* менять строки/столбцы местами.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Приведём пример расширенной матрицы системы. Пусть исходная система будет следующей:\n",
    "\n",
    "$\\left\\{\\begin{array}{c} w_{1}+w_{2}=1 \\\\ w_{1}+2 w_{2}=2 \\end{array}\\right.$\n",
    "\n",
    "Запишем её в матричном виде:\n",
    "\n",
    "$\\left(\\begin{array}{cc} 1 & 1 \\\\ 1 & 2  \\end{array} \\right) \\cdot \\left(\\begin{array}{c} w_1 \\\\ w_2  \\end{array} \\right) = \\left(\\begin{array}{c} 1 \\\\ 2  \\end{array} \\right)$\n",
    "\n",
    "Тогда расширенная матрица системы будет иметь вид:\n",
    "\n",
    "$(A \\mid b)=\\left(\\begin{array}{cc|c} 1 & 2 & 1 \\\\ 1 & 2 & 2 \\\\ \\end{array}\\right)$\n",
    "\n",
    "***\n",
    "\n",
    "Существует три случая при решении неоднородных СЛАУ:\n",
    "\n",
    "* **«Идеальная пара»**\n",
    "\n",
    "Это так называемые определённые системы линейных уравнений, имеющие **единственные решения**.\n",
    "\n",
    "* **«В активном поиске»**\n",
    "\n",
    "Неопределённые системы, имеющие **бесконечно много решений**.\n",
    "\n",
    "* **«Всё сложно»**\n",
    "\n",
    "Это самый интересный для нас случай — переопределённые системы, которые **не имеют точных решений**.\n",
    "\n",
    ">**Примечание**. В данной классификации неоднородных СЛАУ допущено упрощение в терминологии. На самом деле неопределённые системы — это те, в которых независимых уравнений меньше, чем неизвестных. Они могут иметь бесконечно много решений (быть совместными) или ни одного решения (быть несовместными, если уравнения противоречат друг другу).\n",
    ">\n",
    ">На практике, например в обучении регрессий, этот случай практически не встречается.\n",
    ">\n",
    ">Что касается переопределённых систем, то в них, помимо несовместности (отсутствия решений), количество независимых уравнений превышает количество неизвестных — это тот самый случай, что мы видим в регрессионном анализе."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## СЛУЧАЙ «ИДЕАЛЬНАЯ ПАРА»\n",
    "\n",
    ">Самый простой случай решения неоднородной СЛАУ — когда система **имеет единственное решение**. Такие системы называются **совместными**.\n",
    "\n",
    "На вопрос о том, когда СЛАУ является совместной, отвечает главная теорема СЛАУ — теорема **Кронекера — Капелли** (также её называют **критерием совместности системы**).\n",
    "\n",
    "***\n",
    "\n",
    "**Теорема Кронекера — Капелли:**\n",
    "\n",
    "Неоднородная система линейный алгебраических уравнений $A\\overrightarrow{w} = \\overrightarrow{b}$ является совместной тогда и только тогда, когда ранг матрицы системы $A$ равен рангу расширенной матрицы системы $(A|b)$ и равен количеству независимых переменных $m$:\n",
    "\n",
    "$rk(A) = rk(A|\\overrightarrow{b}) = m \\leftrightarrow \\exists ! \\overrightarrow{w} = (w_{1}, w_{2}, \\ldots w_m)^T$\n",
    "\n",
    "Причём решение системы будет равно:\n",
    "\n",
    "$\\overrightarrow{w} = A^{-1} \\overrightarrow{b}$\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Пример №1\n",
    "\n",
    "$\\left\\{\\begin{array}{c} w_{1}+w_{2}=1 \\\\ w_{1}+2 w_{2}=2 \\end{array}\\right.$\n",
    "\n",
    "где  $w_1$ и $w_2$ — неизвестные переменные.\n",
    "\n",
    "При решении системы «в лоб» получим:\n",
    "\n",
    "$\\left\\{\\begin{array}{c} w_{1}+w_{2}=1 \\\\ w_{1}+2 w_{2}=2 \\end{array}\\right. \\Rightarrow \\left\\{\\begin{array}{c} w_{1}+w_{2}=1 \\\\ w_{2}=2 \\end{array}\\right. \\Rightarrow \\left\\{\\begin{array}{c} w_{1}=0 \\\\ w_{2}=0 \\end{array}\\right.$\n",
    "\n",
    "Интерпретация:\n",
    "\n",
    "$\\left( \\begin{array}{c} 1 \\\\ 2 \\end{array}\\right) = 0 \\cdot \\left( \\begin{array}{c} 1 \\\\ 1 \\end{array}\\right) + 1 \\cdot \\left( \\begin{array}{c} 1 \\\\ 2 \\end{array}\\right)$\n",
    "\n",
    "На языке линейной алгебры это означает что вектор $(1, 2)^T$ линейно выражается через векторы коэффициентов системы $(1, 1)^T$ и $(1, 2)^T$.\n",
    "\n",
    "В матричном виде система запишется, как:\n",
    "\n",
    "$A\\overrightarrow{w}=\\overrightarrow{b} \\text{где}A = \\left( \\begin{array}{cc} 1 & 1 \\\\ 1 & 2 \\end{array}\\right), \\overrightarrow{w}=\\left( \\begin{array}{c} w_1 \\\\ w_2 \\end{array}\\right), \\overrightarrow{b}=\\left( \\begin{array}{c} b_1 \\\\ b_2 \\end{array}\\right) $ \n",
    "\n",
    "Преобразование уравнений будем таким же, как и при преобразовании расширенной матрицы системы $(A|b)$, вычитая сначала первую строку из второй, а затем — результат из первой, получим то же решение, что и решение «в лоб».\n",
    "\n",
    "$(A|\\overrightarrow{b})=\\left(\\begin{array}{cc|c} 1 & 1 & 1  \\\\ 1 & 2 & 2 \\end{array} \\right) \\Rightarrow \\left(\\begin{array}{cc|c} 1 & 1 & 1  \\\\ 0 & 1 & 1 \\end{array} \\right) \\Rightarrow \\left(\\begin{array}{cc|c} 1 & 0 & 0  \\\\ 0 & 1 & 1 \\end{array} \\right) \\Rightarrow \\left\\{\\begin{array}{c} w_1=0  \\\\ w_2=0 \\end{array} \\right.$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Других решений у системы нет. \n",
    "\n",
    "Посмотрим на ранги матрицы $А$ и расширенной матрицы $(A|b)$ (количество ступеней в ступенчатых матрицах):\n",
    "\n",
    "$rk(A)=\\left(\\begin{array}{cc} 1 & 0 \\\\ 0 & 1\\end{array}\\right)=2$\n",
    "\n",
    "$rk(A|b)=\\left(\\begin{array}{ccc} 1 & 0 & 0 \\\\ 0 & 1 & 1\\end{array}\\right)=2$\n",
    "\n",
    "$rk(A)=rk(A|b)$\n",
    "\n",
    "Они совпадают и равны количеству неизвестных, а это и гарантирует существование и **единственность решения**. То есть в общем случае, чтобы узнать, сколько решений существует у системы, её необязательно было бы решать — достаточно было бы найти ранги матриц $rk(A)$ и $rk(A|b)$ ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Тут возникает вопрос: «Можно ли найти решение одной формулой?»\n",
    "\n",
    "Для удобства перепишем систему без стрелок:\n",
    "\n",
    "$Aw=b$\n",
    "\n",
    "Так как матрица квадратная и невырожденная, у неё обязательно есть обратная матрица.\n",
    "\n",
    "Умножим на $A^{-1}$ слева обе части уравнения. Стоит напомнить, что произведение матриц **не перестановочно**, поэтому есть разница, с какой стороны умножать.\n",
    "\n",
    "$A^{-1} \\cdot Aw = A^{-1}\\cdot b$\n",
    "\n",
    "$w=A^{-1}\\cdot b$\n",
    "\n",
    ">**Важно**! Отсюда явно видны ограничения этого метода: его можно применять только **для квадратных невырожденных матриц** (тех, у которых определитель не равен 0).\n",
    "\n",
    "Убедимся в правильности формулы. Найдём произведение матрицы $A^{-1}$ и вектора-столбца $b$:\n",
    "\n",
    "$A^{-1}\\cdot b=\\left( \\begin{array}{cc} 1 & 1 \\\\ 1 & 2 \\end{array}\\right)^{-1} \\cdot \\left(\\begin{array}{c} 1 \\\\ 2 \\end{array}\\right)=\\left( \\begin{array}{cc} 2 & -1 \\\\ -1 & 1 \\end{array}\\right) \\cdot \\left(\\begin{array}{c} 1 \\\\ 2 \\end{array}\\right) = \\left(\\begin{array}{c} 0 \\\\ 1 \\end{array}\\right)$ \n",
    "\n",
    "***\n",
    "\n",
    "**Резюмируем ↓**\n",
    "\n",
    "У нас есть квадратная система с $m$ неизвестных. Если ранг матрицы коэффициентов $A$ **равен** рангу расширенной матрицы $(A|b)$ и **равен** количеству переменных $(rk(A)=rk(\\overrightarrow{b}))=m$, то в системе будет ровно столько независимых уравнений, сколько и неизвестных $m$, а значит будет **единственное** решение.\n",
    "\n",
    "Вектор свободных коэффициентов $b$ при этом линейно независим со столбцами матрицы $A$, его разложение по столбцам $A$ **единственно**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## СЛУЧАЙ «В АКТИВНОМ ПОИСКЕ»\n",
    "\n",
    "?А что, если система **не удовлетворяет теореме Кронекера — Капелли**? То есть ранг матрицы системы равен расширенному рангу матрицы, но не равен количеству неизвестных. Неужели тогда система нерешаема?\n",
    "\n",
    "На этот вопрос отвечает первое следствие из теоремы ↓\n",
    "\n",
    "**Следствие №1** из теоремы Кронекера — Капелли:\n",
    "\n",
    "Если ранг матрицы системы $A$ равен рангу расширенной матрицы системы $(A|b)$, **но меньше**, чем количество неизвестных $m$, то система имеет бесконечное множество решений:\n",
    "\n",
    "$rk(A) = rk(A | \\vec{b}) < m  \\leftrightarrow  \\infty \\ решений$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решим систему уравнений:\n",
    "\n",
    "$w_1+w_2+w_3=10$\n",
    "\n",
    "Да, уравнение одно, но формально оно является неоднородной СЛАУ.\n",
    "\n",
    "Итак, мы имеем одно уравнение на три неизвестных, значит две координаты из трёх вектора $w$ мы можем задать как угодно. Например, зададим вторую и третью как $\\alpha$ и $\\beta$. Тогда первая будет равна $10-\\alpha-\\beta$.\n",
    "\n",
    "$w=\\left( \\begin{array}{c} {10-\\alpha-\\beta} \\\\ \\alpha \\\\ \\beta \\end{array} \\right)$ где $\\alpha, \\beta \\in \\mathbb{R}$\n",
    "\n",
    "Вместо переменных $\\alpha$ и $\\beta$ мы можем подставлять любые числа и всегда будем получать равенство. \n",
    "\n",
    "Составим расширенную матрицу:\n",
    "\n",
    "$(A|b)=(\\begin{array}{} 1 & 1 & 1|10 \\end{array})$\n",
    "\n",
    "Её ранг, как и ранг $A$, равен 1, что меньше числа неизвестных $m=3$:\n",
    "\n",
    "$rk(A) = rk(A | \\vec{b}) = 1 < 3$\n",
    "\n",
    "Такая ситуация, по следствию из теоремы Кронекера — Капелли, говорит о существовании и не единственности решения, то есть решений **бесконечно много**.\n",
    "\n",
    "***\n",
    "\n",
    "**Резюмируем ↓\n",
    "**\n",
    "Если ранги матриц $A$ и $(A|b)$ всё ещё совпадают, но уже меньше количества неизвестных $(rk(A) = rk(A | \\vec{b}) < m)$, значит, уравнений не хватает для того, чтобы определить систему полностью, и решений будет бесконечно много.\n",
    "\n",
    "На языке линейной алгебры это значит, что вектор $\\overrightarrow{b}$ линейно зависим со столбцами матрицы $A$, но также и сами столбцы зависимы между собой, поэтому равнозначного разложения не получится, т. е. таких разложений может быть сколько угодно."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## СЛУЧАЙ «ВСЁ СЛОЖНО»\n",
    "\n",
    "А теперь посмотрим на самый интересный для нас случай. Его формально регламентирует второе следствие из теоремы Кронекера — Капелли.\n",
    "\n",
    "**Следствие №2 из теоремы Кронекера — Капелли**:\n",
    "\n",
    "Если ранг матрицы системы $A$ меньше, чем ранг расширенной матрицы системы $(A|b)$, то система несовместна, то есть не имеет точных решений:\n",
    "\n",
    "$rk(A)  < rk(A | \\vec{b})  \\leftrightarrow  \\nexists \\ решений$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решим систему уравнений:\n",
    "\n",
    "$\\left\\{\\begin{array}{c} w_1+w_2=1 \\\\ w_1+2 w_2=2 \\\\ w_1+w_2=12 \\end{array}\\right.$\n",
    "\n",
    "Посмотрим на первое и третье уравнение — очевидно, что такая система не имеет решений, так как данные уравнения противоречат друг другу.\n",
    "\n",
    "Но давайте обоснуем это математически. Для этого запишем расширенную матрицу системы:\n",
    "\n",
    "$(A|b)=\\left(\\begin{array}{cc|c} 1 & 1 & 1 \\\\ 1 & 2 & 2 \\\\ 1 & 1 & 12 \\end{array}\\right)$\n",
    "\n",
    "Посчитаем ранги матриц $A$ и $(A|b)$:\n",
    "\n",
    "$rk(A)=rk\\left(\\begin{array}{cc} 1 & 1  \\\\ 1 & 2 \\\\ 1 & 1 \\end{array}\\right) \\Rightarrow I-III \\Rightarrow rk\\left(\\begin{array}{cc} 1 & 1  \\\\ 1 & 2 \\end{array}\\right) \\Rightarrow II-I \\Rightarrow rk\\left(\\begin{array}{cc} 1 & 1  \\\\ 0 & 1 \\end{array}\\right)=2$\n",
    "\n",
    "$rk(A|b)=rk\\left(\\begin{array}{ccc} 1 & 1 & 1 \\\\ 1 & 2 & 2 \\\\ 1 & 1 & 12 \\end{array}\\right) \\Rightarrow III-I, II-I \\Rightarrow rk\\left(\\begin{array}{cc} 1 & 1 & 1 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 12\\end{array}\\right)=3$\n",
    "\n",
    "Итак, $rk(A)=2$, в то время как $rk(A|b)=3$. Это и есть критерий **переопределённости системы уравнений**: ранг матрицы системы меньше ранга расширенной матрицы системы.\n",
    "\n",
    "Получается, что идеальное решение найти нельзя, но чуть позже мы увидим, что такие системы возникают в задачах регрессии практически всегда, а значит нам всё-таки хотелось бы каким-то образом её решать. Можно попробовать найти приблизительное решение — вопрос лишь в том, какое из всех этих решений лучшее."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем наилучшее приближение для $w_1$, $w_2$, если:\n",
    "\n",
    "$\\left\\{\\begin{array}{c} w_1+w_2=1 \\\\ w_1+2 w_2=2 \\\\ w_1+w_2=12 \\end{array}\\right. \\Rightarrow \\left(\\begin{array}{cc} 1 & 1  \\\\ 1 & 2 \\\\ 1 & 1 \\end{array}\\right)\\cdot \\left(\\begin{array}{c} w  \\\\ w \\end{array}\\right)=\\left(\\begin{array}{c} 1 \\\\ 2 \\\\ 12 \\end{array}\\right)$\n",
    "\n",
    "Обозначим приближённое решение как $\\hat{w}$. Приближением для вектора $b$ будет $\\hat{b}=A \\hat{w}$. Также введём некоторый вектор ошибок $e=b-\\hat{b}=b-A \\hat{w}$.\n",
    "\n",
    ">**Примечание**. Здесь мы снова опустили стрелки у векторов $b$, $\\hat{b}$ и $\\hat{w}$ для наглядности.\n",
    "\n",
    "Например, если мы возьмём в качестве вектора $\\hat{w}$ вектор $\\hat{w_1}=(1,1)^T$, то получим:\n",
    "\n",
    "$\\hat{b}=A\\hat{w_1}=\\left(\\begin{array}{cc} 1 & 1 \\\\ 1 & 2 \\\\ 1 & 1\\end{array}\\right)\\cdot\\left(\\begin{array}{c} 1 \\\\ 1  \\end{array}\\right)=\\left(\\begin{array}{c} 2 \\\\ 3 \\\\ 2 \\end{array}\\right)$\n",
    "\n",
    "$e_1=b-A\\hat{w_1}=\\left(\\begin{array}{c} 1 \\\\ 2 \\\\ 12\\end{array}\\right) - \\left(\\begin{array}{c} 2 \\\\ 3 \\\\ 2\\end{array}\\right)=\\left(\\begin{array}{c} -1 \\\\ -1 \\\\ 10 \\end{array}\\right)$\n",
    "\n",
    "Теперь возьмём в качестве вектора $\\hat{w_2}=(4, -1)^T$, получим:\n",
    "\n",
    "$\\hat{b}=A\\hat{w_2}=\\left(\\begin{array}{cc} 1 & 1 \\\\ 1 & 2 \\\\ 1 & 1\\end{array}\\right)\\cdot\\left(\\begin{array}{c} 4 \\\\ -1  \\end{array}\\right)=\\left(\\begin{array}{c} 3 \\\\ 2 \\\\ 3 \\end{array}\\right)$\n",
    "\n",
    "$e_2=b-A\\hat{w_2}=\\left(\\begin{array}{c} 1 \\\\ 2 \\\\ 12\\end{array}\\right) - \\left(\\begin{array}{c} 3 \\\\ 2 \\\\ 3\\end{array}\\right)=\\left(\\begin{array}{c} -2 \\\\ -1 \\\\ 9 \\end{array}\\right)$\n",
    "\n",
    ">Конечно, нам хотелось бы, чтобы ошибка была поменьше. Но какая из них поменьше? Векторы сами по себе сравнить нельзя, **но зато можно сравнить их длины**.\n",
    "\n",
    "$\\left\\|e_1 \\right\\| = \\sqrt{(-1)^2 + (-1)^2 + (10)^2} = \\sqrt{102}$\n",
    "\n",
    "$\\left\\|e_2 \\right\\| = \\sqrt{(-2)^2 + 0^2 + 9^2} = \\sqrt{85}$\n",
    "\n",
    ">Видно, что вторая ошибка всё-таки меньше, соответственно, приближение лучше. Но в таком случае из всех приближений нам нужно выбрать то, у которого длина вектора ошибок минимальна, если, конечно, это возможно.\n",
    ">\n",
    ">$||e||\\rightarrow min$\n",
    ">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    ">**Примечание**. Проблема поиска оптимальных приближённых решений неоднородных переопределённых СЛАУ стояла у математиков вплоть до XIX века. До этого времени математики использовали частные решения, зависящие от вида уравнений и размерности. Впервые данную задачу для общего случая решил Гаусс, опубликовав метод решения этой задачи, который впоследствии будет назван методом наименьших квадратов (МНК). В дальнейшем Лаплас прибавил к данному методу теорию вероятности и доказал оптимальность МНК-оценок с точки зрения статистики.\n",
    "\n",
    "Сейчас мы почувствуем себя настоящими математиками и попробуем решить эту задачу самостоятельно с помощью простой геометрии и знакомых нам операций над матрицами.\n",
    "\n",
    "Вспомним, что на языке линейной алгебры неразрешимость системы\n",
    "\n",
    "$\\left(\\begin{array}{cc} 1 & 1  \\\\ 1 & 2 \\\\ 1 & 1 \\end{array}\\right)\\cdot \\left(\\begin{array}{c} w  \\\\ w \\end{array}\\right)=\\left(\\begin{array}{c} 1 \\\\ 2 \\\\ 12 \\end{array}\\right)$\n",
    "\n",
    "означает, что попытка выразить вектор $(1,2,12)^T$ через $(1,1,1)^T$ и $(1,2,1)^T$ не будет успешной, так как они **линейно независимы**.\n",
    "\n",
    "Геометрически это означает, что вектор свободных коэффициентов $\\textcolor{brown}{b}$ (коричневый) не лежит в одной плоскости со столбцами матрицы $\\textcolor{blue}{A}$ (синие векторы).\n",
    "\n",
    "<img src=m2_img1.png>\n",
    "\n",
    "Идея состояла в том, что наилучшим приближением для коричневого вектора будет ортогональная проекция на синюю плоскость — $\\textcolor{cyan}{голубой}$ вектор. Так происходит потому, что наименьший по длине вектор ошибок — $\\textcolor{red}{красный}$ — должен быть перпендикулярен к синей плоскости:\n",
    "\n",
    "$e=b-\\hat{b}$\n",
    "\n",
    "В прошлом модуле мы производили расчёты интуитивно, а теперь настала пора вывести формулу.\n",
    "\n",
    "Давайте умножим наши уравнения слева на $A^T$:\n",
    "\n",
    "$A^T\\cdot\\textcolor{cyan}{A\\hat{w}}=A^T\\cdot\\textcolor{brown}{b}$\n",
    "\n",
    "Идея заключается в следующем: справа мы найдём скалярное произведение столбцов матрицы $A$ на вектор $b$, а слева — произведение столбцов $A$ на приближённый вектор $\\hat{b}$ (по сути, на голубую проекцию).\n",
    "\n",
    "Упростим уравнение, перемножив всё, что содержит только числа. В левой части умножим $A^T$ на $A$, в правой — умножим $A^T$ на $b$. Тогда слева получим матрицу 2×2 — это не что иное, как матрица Грама столбцов $A$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Столбцы $A$ линейно независимы, а это значит, что, по свойству матрицы Грама, $A^T\\cdot A$  — невырожденная квадратная матрица (её определитель не равен нулю, и для неё существует обратная матрица). Получившаяся система — один в один случай «идеальная пара» (ранг матрицы, как и ранг расширенной матрицы, равен 2, в чём несложно убедиться), а это значит, что теперь мы можем её решить.\n",
    "\n",
    "$\\left(\\begin{array}{} 3 & 4 \\\\ 4 & 6 \\end{array}\\right)\\cdot\\left(\\begin{array}{} \\hat{w} \\\\ \\hat{w} \\end{array}\\right) = \\left(\\begin{array}{} 15 \\\\ 17 \\end{array}\\right)$\n",
    "\n",
    "$\\left(\\begin{array}{} 3 & 4 \\\\ 4 & 6 \\end{array}\\right)$ - матрица Грамма столбцов $A$\n",
    "\n",
    "$A^T\\cdot A=Gram(\\left(\\begin{array}{} 1 \\\\ 1 \\\\ 1 \\end{array}\\right),\\left(\\begin{array}{} 1 \\\\ 2 \\\\ 1 \\end{array}\\right))$\n",
    "\n",
    "Но ведь мы не могли решить изначальную задачу, так как она была переопределена, а эту — можем. **Как так получилось**?\n",
    "\n",
    "Мы потребовали, чтобы у приближения $\\hat{b}$ были с векторами $(1,1,1)^T$ и $(1,2,1)^T$ такие же скалярные произведения, как у $b$. Это и означает что $\\hat{b}$ — ортогональная проекция на нашу синюю плоскость, в которой лежат столбцы матрицы $A$, и в этой плоскости мы можем найти коэффициенты.\n",
    "\n",
    "Мы с вами отлично умеем решать системы типа «Идеальная пара». Для этого нам нужно найти обратную матрицу $(A^T\\cdot A)^{-1}$ и умножить на неё слева всё уравнение. Так мы и получим наше приближение:\n",
    "\n",
    "$(A^TA)\\cdot\\hat{w}=A^Tb$\n",
    "\n",
    "Находим определитель матрицы $(A^TA)$: \n",
    "\n",
    "$\\mathbb{det}(A^TA) = 3\\cdot6-4\\cdot4=2$\n",
    "\n",
    "Находим обратную матрицу $(A^TA)^{-1}$:\n",
    "\n",
    "$(A^TA)^{-1}=\\left(\\begin{array}{cc} 3 & 4 \\\\ 4 & 6\\end{array}\\right)^{-1}=\\frac{1}{2}\\left(\\begin{array}{cc} 6 & -4 \\\\ -4 & 3\\end{array}\\right)=\\left(\\begin{array}{cc} 3 & -2 \\\\ -2 & 1.5\\end{array}\\right)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Умножаем всё уравнение на обратную матрицу слева:\n",
    "\n",
    "$(A^TA)^{-1}\\cdot(A^TA)\\cdot\\hat{w}=(A^TA)^{-1}\\cdot A^T\\cdot b$\n",
    "\n",
    "$\\hat{w}=(A^TA)^{-1}\\cdot A^T\\cdot b$\n",
    "\n",
    "И, наконец, вот он — долгожданный приближённый вектор $\\hat{w}$:\n",
    "\n",
    "$\\hat{w}=\\left(\\begin{array}{cc} 3 & -2 \\\\ -2 & 1.5\\end{array}\\right)\\cdot\\left(\\begin{array}{} 15 \\\\ 17\\end{array}\\right)=\\left(\\begin{array}{} 11 \\\\ -4.5\\end{array}\\right)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "⭐ **Пришло время открытий!**\n",
    "\n",
    "Только что мы геометрическим образом вывели формулу оценки решения методом наименьших квадратов (`МНК` или `OLS`, Ordinary Least Squares).\n",
    "\n",
    ">**Примечание**. Стоит отметить, что полученная матричная формула не зависит от размерностей и конкретных значений, а значит применима не только в нашем локальном случае, но и в общем.\n",
    "\n",
    "Нам осталось выполнить проверку полученных результатов, чтобы убедиться в верности решения.\n",
    "\n",
    "Вычислим голубой вектор $\\hat{b}$. Для этого возьмём линейную комбинацию столбцов матрицы $А$ с найденными нами коэффициентами $\\hat{w_1}$ и $\\hat{w_2}$\n",
    "\n",
    "$\\hat{b}=A\\hat{w}=\\hat{w_1}\\cdot\\left(\\begin{array}{} 1\\\\1\\\\1\\end{array}\\right)+\\hat{w_2}\\cdot\\left(\\begin{array}{} 1\\\\2\\\\1\\end{array}\\right)=11\\cdot\\left(\\begin{array}{} 1\\\\1\\\\1\\end{array}\\right)-4.5\\cdot\\left(\\begin{array}{} 1\\\\2\\\\1\\end{array}\\right)=\\left(\\begin{array}{} 6.5\\\\2\\\\6.5\\end{array}\\right)$\n",
    "\n",
    "Вычислим вектор ошибок $e$:\n",
    "\n",
    "$e=b-\\hat{b}=b-A\\hat{w}=\\left(\\begin{array}{} 1\\\\2\\\\12\\end{array}\\right)-\\left(\\begin{array}{} 6.5\\\\2\\\\6.5\\end{array}\\right)=\\left(\\begin{array}{} -5.5\\\\0\\\\5.5\\end{array}\\right)$\n",
    "\n",
    "Убедимся, что данный вектор действительно ортогонален столбцам матрицы $А$. Для этого найдём их скалярные произведения:\n",
    "\n",
    "$(e,A_1)=e^T\\cdot A_1=(-5.5, 0, 5.5) \\cdot \\left(\\begin{array}{} 1\\\\1\\\\1\\end{array}\\right) = 0$\n",
    "\n",
    "$(e,A_2)=e^T\\cdot A_2=(-5.5, 0, 5.5) \\cdot \\left(\\begin{array}{} 1\\\\2\\\\1\\end{array}\\right) = 0$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скалярные произведения равны 0, а это означает, что вектор ошибок $\\textcolor{red}e$ действительно ортогонален всей синей плоскости, а голубой вектор $\\textcolor{cyan}{\\hat{b}}$ приближённого значения является ортогональной проекцией коричневого вектора $\\textcolor{brown}b$.\n",
    "\n",
    ">**Примечание**. Прежде чем перейти к выводам, стоит отметить, что обычно `OLS`-оценку выводят немного иначе, а именно минимизируя в явном виде длину вектора ошибок по коэффициентам $\\hat{w}$, вернее, даже квадрат длины для удобства вычисления производных.\n",
    ">\n",
    ">$||\\overrightarrow{e}||\\rightarrow min$\n",
    ">\n",
    ">$||\\overrightarrow{e}||^2\\rightarrow min$\n",
    ">\n",
    ">$||\\overrightarrow{b}-A\\overrightarrow{w}||^2\\rightarrow min$\n",
    ">\n",
    ">Формула получится точно такой же, какая есть у нас, просто способ вычислений будет не геометрический, а аналитический. Мы вернёмся к этому способу, когда будем обсуждать оптимизацию функции многих переменных в разделе по математическому анализу.\n",
    "\n",
    "Наконец, мы может подвести итоги для случая «Всё сложно»."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Резюмируем ↓**\n",
    "\n",
    "Если ранг матрицы $A$ меньше ранга расширенной системы $(A|b)$, то независимых уравнений больше, чем переменных $(rkA<(A|b)<m)$, а значит некоторые из них будут противоречить друг другу, то есть решений у системы нет.\n",
    "\n",
    "Говоря на языке линейной алгебры, вектор $b$ линейно независим со столбцами матрицы $A$, а значит его нельзя выразить в качестве их линейной комбинации.\n",
    "\n",
    "Однако можно получить приближённые решения по методу наименьших квадратов (`OLS` - оценка - $\\hat{b}=(A^TA)^{-1}\\cdot A^Tb$), идеей которого является ортогональная проекция вектора $b$ на столбцы матрицы $A$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдите `OLS`-оценку для коэффициентов $w_1$, $w_2$ СЛАУ:\n",
    "\n",
    "$\\left\\{\\begin{array}{c} w_1+2w_2=1 \\\\ -3w_1+w_2=4 \\\\ w_1+2w_2=5 \\\\ w_1-w_2=0\\end{array}\\right.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "A = np.array([[1,2],[-3,1],[1,2],[1,-1]])\n",
    "b = np.array([1,4,5,0])\n",
    "b = np.reshape(b,(4,1))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Вычислите матрицу Грама столбцов $A:A^{T}A=$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12,  0],\n",
       "       [ 0, 10]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T@A"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Вычислите матрицу $(A^{T}A)^{-1}$. Она имеет вид:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08333333, 0.        ],\n",
       "       [0.        , 0.1       ]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_a = np.linalg.inv(A.T@A)\n",
    "a_a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Вычислите $A^{T} \\overrightarrow{b}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6],\n",
       "       [16]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_b = A.T@b\n",
    "a_b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Вычислите вектор оценок коэффициентов $\\overrightarrow{w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5],\n",
       "       [ 1.6]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_a@a_b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Линейная регрессия МНК <a class=\"anchor\" id=2></a>\n",
    "\n",
    "[к содержанию](#0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задаче регрессии обычно есть **целевая переменная**, которую мы хотим предсказать. Её, как правило, обозначают буквой $y$. Помимо целевой переменной, есть **признаки** (их также называют **факторами** или **регрессорами**). Пусть их будет $k$ штук:\n",
    "\n",
    "$y$ — целевая переменная\n",
    "\n",
    "$x_1, x_2, \\dots, x_k$ — признаки/факторы/регрессоры\n",
    "\n",
    "Поставить задачу — значит ответить на два вопроса:\n",
    "\n",
    "1. Что у нас есть?\n",
    "\n",
    "2. Что мы хотим получить?\n",
    "\n",
    "В задаче регрессии есть $N$ (как правило, их действительно много) наблюдений. Это наша обучающая выборка или датасет, представленный в виде таблицы. В столбцах таблицы располагаются векторы признаков $\\overrightarrow{x_i}$.\n",
    "\n",
    "$\\overrightarrow{y} \\in \\mathbb{R}^N$\n",
    "\n",
    "$x_1, x_2, \\dots, x_k\\in \\mathbb{R}^N$\n",
    "\n",
    "$\\left(\\begin{array}{} y_1 \\\\ y_2 \\\\ \\dots \\\\ y_N\\end{array}\\right), \\left(\\begin{array}{} x_1 \\\\ x_2 \\\\ \\dots \\\\ x_N\\end{array}\\right), \\dots, \\left(\\begin{array}{} x_k1 \\\\ x_k2 \\\\ \\dots \\\\ x_kN\\end{array}\\right)$\n",
    "\n",
    "То есть и целевая переменная, и признаки представлены векторами из векторного пространства $\\mathbb{R}^N$ — каждого вектора $N$ координат.\n",
    "\n",
    "В качестве регрессионной модели мы будем использовать **модель линейной регрессии**. Мы предполагаем, что связь между целевой переменной и признаками линейная. Это означает, что:\n",
    "\n",
    "$y=w_0+w_1x_1+w_2x_2+…+w_kx_k,$\n",
    "\n",
    "или \n",
    "\n",
    "$y=(\\overrightarrow{w}, \\overrightarrow{x})$\n",
    "\n",
    "Здесь $\\overrightarrow{w}=(w_0,w_1,…,w_k)^T$ обозначают веса (коэффициенты уравнения линейной регрессии), а $\\overrightarrow{x}=(1,x_1, x_2,…, x_k)^T$.\n",
    "\n",
    ">Наличие коэффициента $w_0$ говорит о том, что мы строим регрессию с константой, или, как ещё иногда говорят, с **интерсептом**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пока что коэффициенты $w$ нам неизвестны. Как же их найти?\n",
    "\n",
    "Для этого у нас есть $N$ наблюдений — обучающий набор данных.\n",
    "\n",
    "Давайте попробуем подобрать такие веса $w$, чтобы для каждого наблюдения наше равенство было выполнено. Таким образом, получается $N$ уравнений на $k+1$ неизвестную.\n",
    "\n",
    "$\\left(\\begin{array}{} y_1 \\\\ y_2 \\\\ \\dots \\\\ y_N\\end{array}\\right)=w_0\\cdot \\left(\\begin{array}{} 1 \\\\ 2 \\\\ \\dots \\\\ 1 \\end{array}\\right)+w_1 \\cdot\\left(\\begin{array}{} x_11 \\\\ x_12 \\\\ \\dots \\\\ x_1N\\end{array}\\right)+ \\dots +w_k\\cdot \\left(\\begin{array}{} x_k1 \\\\ x_k2 \\\\ \\dots \\\\ x_kN\\end{array}\\right)$\n",
    "\n",
    "Или в привычном виде систем уравнений:\n",
    "\n",
    "$\\left\\{\\begin{array}{} w_0 1+w_1 x_{11} + \\dots +w_k x_{k1}=y_1 \\\\ w_0 1+w_1 x_{12} + \\dots +w_k x_{k2}=y_2 \\\\ \\dots \\\\ w_0 1+w_1 x_{1N} + \\dots +w_k x_{kN}=y_N \\end{array}\\right.$\n",
    "\n",
    ">Говоря на языке машинного обучения, мы хотим обучить такую модель, которая описывала бы зависимость целевой переменной от факторов на обучающей выборке.\n",
    "\n",
    "Как правило, $N$ гораздо больше $k$ (количество строк с данными в таблице намного больше количества столбцов) и система переопределена, значит точного решения нет. Поэтому можно найти только приближённое."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Примечание**. Полученной СЛАУ можно дать геометрическую интерпретацию. Если представить каждое наблюдение в виде точки на графике (см. ниже), то уравнение линейной регрессии будет задавать прямую (если фактор один) или гиперплоскость (если факторов $k$ штук). Приравняв уравнение прямой к целевому признаку, мы потребовали, чтобы эта прямая проходила через все точки в нашем наборе данных. Конечно же, это условие не может быть выполнено полностью, так как в данных всегда присутствует какой-то шум, и идеальной прямой (гиперплоскости) не получится, но зато можно построить приближённое решение.\n",
    ">\n",
    "><img src=m2_img2.png width=400>\n",
    ">\n",
    ">**Обратите внимание**, что у нас появился новый вектор из единиц. Он здесь из-за того, что мы взяли модель с интерсептом. Можно считать, что это новый регрессор-константа. Данная константа тянется из уравнения прямой, которое мы разбирали в модуле «ML-2. Обучение с учителем: регрессия».\n",
    "\n",
    "Мы уже умеем решать переопределённые системы, для этого мы должны составить матрицу системы $A$, записав в столбцы все наши регрессоры, включая регрессор константу:\n",
    "\n",
    "$A=\\left(\\begin{array}{} 1 & x_{11} & \\dots & x_{k1} \\\\ 1 & x_{12} & \\dots & x_{k2} \\\\ \\dots & \\dots & \\dots & \\dots \\\\ 1 & x_{1N} & \\cdot & x_{kN} \\end{array}\\right) \\Rightarrow N - строк, k+1 - столбец$\n",
    "\n",
    ">**Примечание**. В контексте задач машинного обучения матрица $A$ называется **матрицей наблюдений**: по строкам отложены наблюдения (объекты), а по столбцам — характеризующие их признаки. В модулях по машинному обучению мы в основном обозначали её за $X$. Здесь же мы будем придерживаться традиций линейной алгебры и обозначать матрицу за $A$.\n",
    ">\n",
    ">**Примечание**. Обратите внимание, что индексация матрицы $A$ отличается от привычной нам индексации матрицы. Например, здесь $x_{12}$ — второе наблюдение первого регрессора. Это чистая формальность. Если обозначать за первый индекс номер наблюдения, а за второй индекс — номер регрессора, мы получим привычную нам нумерацию элементов матрицы (строка-столбец)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось записать финальную формулу `OLS`-оценки для коэффициентов:\n",
    "\n",
    "$\\hat{\\overrightarrow{w}} = (A^T A)^{-1} A^T \\overrightarrow{y}$\n",
    "\n",
    "Казалось бы, задача решена, однако это совсем не так, ведь мы искали коэффициенты не просто так, а чтобы сделать прогноз — предсказание на новых данных.\n",
    "\n",
    "Допустим, у нас есть новое наблюдение по регрессорам, которое характеризуется признаками $\\overrightarrow{x}_{NEW} = (x_{1, NEW}, x_{2, NEW}, ..., x_{k, NEW})^T$. Тогда, предсказание будет строиться следующим образом:\n",
    "\n",
    "$\\vec{y}_{NEW} = \\vec{w}_0 + \\vec{w}_1 x_{1, NEW} + ... + \\vec{w}_k x_{k, NEW}$\n",
    "\n",
    "или\n",
    "\n",
    "$\\vec{y}_{NEW} = (\\hat{\\vec{w}}, \\vec{x}_{NEW})$\n",
    "\n",
    "Теперь перейдём от формул к практике и решим задачу в контексте."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "Рассмотрим классический датасет для обучения линейной регрессии — `Boston Housing`. В нём собраны усреднённые данные по стоимости недвижимости в 506 районах Бостона. Ниже вы видите фрагмент датасета.\n",
    "\n",
    "Целевой переменной будет $\\textcolor{red}{PRICE}$ — это, в некотором смысле, типичная (медианная) стоимость дома в районе.\n",
    "\n",
    "Для примера возьмём в качестве регрессоров уровень преступности $\\textcolor{blue}{CRIM}$ и среднее количество комнат в доме $\\textcolor{blue}{RM}$.\n",
    "\n",
    "<img src=m2_img3.png width=600>\n",
    "\n",
    "Запишем нашу модель:\n",
    "\n",
    "$y=w_0+w_1 \\cdot x_1+w_2 \\cdot x_2$\n",
    "\n",
    "Для наглядности обозначим:\n",
    "\n",
    "$y=w_0+w_1 \\cdot CRIM+w_2 \\cdot RM$\n",
    "\n",
    "Составим матрицу регрессоров:\n",
    "\n",
    "$A=\\left(\\begin{array}{} 1 & CRIM_1 & RM_1 \\\\ 1 & CRIM_2 & RM_2 \\\\ \\dots & \\dots & \\dots \\\\ 1 & CRIM_N & RM_N \\end{array}\\right)$\n",
    "\n",
    "В нашем случае $N=506$, а $k=2$. Размерность матрицы $A$ будет равна $\\mathbb{dim}A=(506,3)$. Далее мы применяем формулу для вычисления оценок коэффициентов:\n",
    "\n",
    "$\\hat{\\vec{w}} = (A^T A)^{-1} A^T \\vec{y}$\n",
    "\n",
    "Вычисления к этой задаче мы сделаем в `Python` ниже, а пока приведём конечный результат. Если сократить запись до двух знаков после точки, получим следующие коэффициенты:\n",
    "\n",
    "$\\hat{\\vec{w}} = (-29.3, \\ -0.26, \\ 8.4)^T$\n",
    "\n",
    "То есть:\n",
    "\n",
    "$\\hat{w}_0 = -29.3$\n",
    "\n",
    "$\\hat{w}_1 = -0.26$\n",
    "\n",
    "$\\hat{w}_2 = 8.4$\n",
    "\n",
    "Мы можем переписать нашу модель для прогноза:\n",
    "\n",
    "$\\hat{y} = -29.3 - 0.26 \\cdot CRIM + 8.4 \\cdot RM$\n",
    "\n",
    "Теперь, если у нас появятся новые наблюдения, то есть ещё один небольшой район с уровнем преступности 0.1 на душу населения и средним количеством комнат в доме, равным 8, мы сможем сделать прогноз на типичную стоимость дома в этом районе — 37 тысяч долларов:\n",
    "\n",
    "$CRIM_{NEW} = 0.1$\n",
    "\n",
    "$RM_{NEW} = 8$\n",
    "\n",
    "$\\hat{y}_{NEW} = -29.3 -0.26 \\cdot 0.1 + 8.4 \\cdot 8 \\approx 37$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "## → Решение на Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AubakirovMA\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  PRICE  \n",
       "0     15.3  396.90   4.98   24.0  \n",
       "1     17.8  396.90   9.14   21.6  \n",
       "2     17.8  392.83   4.03   34.7  \n",
       "3     18.7  394.63   2.94   33.4  \n",
       "4     18.7  396.90   5.33   36.2  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузка библиотек\n",
    "import numpy as np # для работы с массивами\n",
    "import pandas as pd # для работы с DataFrame \n",
    "from sklearn import datasets # для импорта данных\n",
    "import seaborn as sns # для визуализации статистических данных\n",
    "import matplotlib.pyplot as plt # для построения графиков\n",
    "\n",
    "# загружаем датасет\n",
    "boston = datasets.load_boston()\n",
    "boston_data = pd.DataFrame(\n",
    "    data=boston.data, #данные\n",
    "    columns=boston.feature_names #наименования столбцов\n",
    ")\n",
    "boston_data['PRICE'] = boston.target\n",
    "boston_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формируем матрицу $A$ из столбца единиц и факторов $CRIM$ и $RM$, а также вектор целевой переменной $y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0000e+00 6.3200e-03 6.5750e+00]\n",
      " [1.0000e+00 2.7310e-02 6.4210e+00]\n",
      " [1.0000e+00 2.7290e-02 7.1850e+00]\n",
      " ...\n",
      " [1.0000e+00 6.0760e-02 6.9760e+00]\n",
      " [1.0000e+00 1.0959e-01 6.7940e+00]\n",
      " [1.0000e+00 4.7410e-02 6.0300e+00]]\n"
     ]
    }
   ],
   "source": [
    "# составляем матрицу А и вектор целевой переменной\n",
    "CRIM = boston_data['CRIM']\n",
    "RM = boston_data['RM']\n",
    "A = np.column_stack((np.ones(506), CRIM, RM))\n",
    "y = boston_data[['PRICE']]\n",
    "print(A)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на размерность матрицы $A$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 3)\n"
     ]
    }
   ],
   "source": [
    "# проверим размерность\n",
    "print(A.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нам ничего не мешает вычислить оценку вектора коэффициентов $w$ по выведенной нами формуле МНК:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-29.24471945]\n",
      " [ -0.26491325]\n",
      " [  8.39106825]]\n"
     ]
    }
   ],
   "source": [
    "# вычислим OLS-оценку для коэффициентов\n",
    "w_hat = np.linalg.inv(A.T@A)@A.T@y\n",
    "print(w_hat.values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь составим прогноз нашей модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37.85733519]\n"
     ]
    }
   ],
   "source": [
    "# добавились новые данные:\n",
    "CRIM_new = 0.1\n",
    "RM_new = 8\n",
    "# делаем прогноз типичной стоимости дома\n",
    "PRICE_new = w_hat.iloc[0]+w_hat.iloc[1]*CRIM_new+w_hat.iloc[2]*RM_new\n",
    "print(PRICE_new.values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Согласитесь, такая запись вычисления оценки стоимости слишком длинная и неудобная, особенно если факторов не два, как у нас, а 200. Более короткий способ сделать прогноз — вычислить скалярное произведение вектора признаков и коэффициентов регрессии.\n",
    "\n",
    "Для удобства дальнейшего использования оформим характеристики нового наблюдения в виде матрицы размером $(1, 3)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: [[37.85733519]]\n"
     ]
    }
   ],
   "source": [
    "new=np.array([[1,CRIM_new,RM_new]])\n",
    "print('prediction:', (new@w_hat).values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Примечание**. Обратите внимание, что, решая задачу с помощью `Python`, мы получили немного другой результат прогноза стоимости. Это связано с тем, что при выполнении ручного расчёта мы округлили значения коэффициентов и получили менее точный результат.\n",
    "\n",
    "Мы уже знаем, что алгоритм построения модели линейной регрессии по МНК реализован в классе `LinearRegression`, находящемся в модуле `sklearn.linear_model`. Для вычисления коэффициентов (обучения модели) нам достаточно передать в метод `fit()` нашу матрицу с наблюдениями и вектор целевой переменной, а для построения прогноза — вызвать метод `predict()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_hat: [[-29.24471945  -0.26491325   8.39106825]]\n",
      "prediction: [[37.85733519]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# создаём модель линейной регрессии\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "# вычисляем коэффициенты регрессии\n",
    "model.fit(A, y)\n",
    "print('w_hat:', model.coef_)\n",
    "new_prediction = model.predict(new)\n",
    "print('prediction:', new_prediction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Примечание. Здесь при создании объекта класса `LinearRegression` мы указали `fit_itercept=False`, так как в нашей матрице наблюдений $A$ уже присутствует столбец с единицами для умножения на свободный член $w_0$. Его повторное добавление не имеет смысла."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделайте прогноз типичной стоимости (в тыс. долларов) дома в городе с уровнем преступности $CRIM=0.2$ и средним количеством комнат в доме $RM=6$. В качестве модели используйте линейную регрессию, оценка вектора коэффициентов которой равна: $\\hat{\\vec{w}} = (-29.3, \\ -0.26, \\ 8.4)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21.04870738]\n"
     ]
    }
   ],
   "source": [
    "# добавились новые данные:\n",
    "CRIM_new = 0.2\n",
    "RM_new = 6\n",
    "# делаем прогноз типичной стоимости дома\n",
    "PRICE_new = w_hat.iloc[0]+w_hat.iloc[1]*CRIM_new+w_hat.iloc[2]*RM_new\n",
    "print(PRICE_new.values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ПРОБЛЕМЫ В КЛАССИЧЕСКОЙ МНК-МОДЕЛИ\n",
    "\n",
    "Заметим, что в уравнении классической `OLS`-регрессии присутствует очень важный множитель $A^TA$:\n",
    "\n",
    "$\\vec{w}=\\left(A^{T} A\\right)^{-1} A^{T}\\vec{y}$\n",
    "\n",
    "Вы могли заметить, что это матрица Грама значений наших признаков, включая признак-константу.\n",
    "\n",
    "Вспомним свойства этой матрицы: \n",
    "\n",
    "* квадратная (размерности $k+1$ на $K+1$, где $k$ — количество факторов);\n",
    "\n",
    "* симметричная.\n",
    "\n",
    ">Как и у любого метода, у классической `OLS`-регрессии есть свои **ограничения**. Если матрица $A^TA$ вырождена или близка к вырожденной, то хорошего решения у классической модели не получится. Такие данные называют **плохо обусловленными**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Корректна ли модель классической `OLS`-регрессии, если\n",
    "\n",
    "$\\vec{y}=\\left(\\begin{array}{} 1\\\\2\\\\5\\\\1\\end{array}\\right), \\vec{x_1}=\\left(\\begin{array}{} 2\\\\1\\\\1\\\\2\\end{array}\\right), \\vec{x_2}=\\left(\\begin{array}{} -2\\\\-1\\\\-1\\\\-2\\end{array}\\right)$\n",
    "\n",
    "Запишем матрицу $A$ и вычислим $A^TA$:\n",
    "\n",
    "$A=(\\vec{1}, \\vec{x_1}, \\vec{x_2})=\\left(\\begin{array}{} 1&2&-2\\\\1&1&-1\\\\1&1&-1\\\\1&2&-2\\end{array}\\right)$\n",
    "\n",
    "$A^TA=\\left(\\begin{array}{} 1&1&1&1\\\\2&1&1&2\\\\-2&-1&-1&-2\\end{array}\\right)\\cdot\\left(\\begin{array}{} 1&2&-2\\\\1&1&-1\\\\1&1&-1\\\\1&2&-2\\end{array}\\right)=\\left(\\begin{array}{} 4&6&-6\\\\6&10&-10\\\\-6&-10&10\\end{array}\\right)$\n",
    "\n",
    "Как видите, две последние строки матрицы $A^TA$ являются пропорциональными. Это говорит о том, что матрица вырождена $(det A^T A =0)$ или её ранг $(rkA)$ меньше количества неизвестных $(3)$, а значит обратной матрицы $(A^TA)^{-1}$ к ней не существует. Отсюда следует, что классическая `OLS`-модель **неприменима для этих данных**.\n",
    "\n",
    ">Борьба с вырожденностью матрицы $A^TA$ часто сводится к устранению «плохих» (зависимых) признаков. Для этого анализируют корреляционную матрицу признаков или матрицу их значений. Но иногда проблема может заключаться, например, в том, что один признак измерен в тысячных долях, а другой — в тысячах единиц. Тогда коэффициенты при них могут отличаться в миллион раз, что потенциально может привести к вырожденности матрицы $A^TA$.\n",
    "\n",
    "В устранении этой проблемы может помочь знакомая нам **нормализация/стандартизация данных**.\n",
    "\n",
    "## ОСОБЕННОСТИ КЛАССА LINEAR REGRESSION БИБЛИОТЕКИ SKLEARN\n",
    "\n",
    "Давайте посмотрим, что «скажет» `Python`, если мы попробуем построить модель линейной регрессии на вырожденной матрице наблюдений, используя классическую формулу линейной регрессии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Личные документы\\Python\\Py\\math&ml-2\\math&ml-2_lecture.ipynb Cell 56\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/%D0%9B%D0%B8%D1%87%D0%BD%D1%8B%D0%B5%20%D0%B4%D0%BE%D0%BA%D1%83%D0%BC%D0%B5%D0%BD%D1%82%D1%8B/Python/Py/math%26ml-2/math%26ml-2_lecture.ipynb#Y130sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m1\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/%D0%9B%D0%B8%D1%87%D0%BD%D1%8B%D0%B5%20%D0%B4%D0%BE%D0%BA%D1%83%D0%BC%D0%B5%D0%BD%D1%82%D1%8B/Python/Py/math%26ml-2/math%26ml-2_lecture.ipynb#Y130sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# вычислим OLS-оценку для коэффициентов\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/%D0%9B%D0%B8%D1%87%D0%BD%D1%8B%D0%B5%20%D0%B4%D0%BE%D0%BA%D1%83%D0%BC%D0%B5%D0%BD%D1%82%D1%8B/Python/Py/math%26ml-2/math%26ml-2_lecture.ipynb#Y130sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m w_hat\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49minv(A\u001b[39m.\u001b[39;49mT\u001b[39m@A\u001b[39;49m)\u001b[39m@A\u001b[39m\u001b[39m.\u001b[39mT\u001b[39m@y\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/%D0%9B%D0%B8%D1%87%D0%BD%D1%8B%D0%B5%20%D0%B4%D0%BE%D0%BA%D1%83%D0%BC%D0%B5%D0%BD%D1%82%D1%8B/Python/Py/math%26ml-2/math%26ml-2_lecture.ipynb#Y130sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(w_hat)\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36minv\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\AubakirovMA\\Anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py:545\u001b[0m, in \u001b[0;36minv\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    543\u001b[0m signature \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mD->D\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m isComplexType(t) \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39md->d\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    544\u001b[0m extobj \u001b[39m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[1;32m--> 545\u001b[0m ainv \u001b[39m=\u001b[39m _umath_linalg\u001b[39m.\u001b[39;49minv(a, signature\u001b[39m=\u001b[39;49msignature, extobj\u001b[39m=\u001b[39;49mextobj)\n\u001b[0;32m    546\u001b[0m \u001b[39mreturn\u001b[39;00m wrap(ainv\u001b[39m.\u001b[39mastype(result_t, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m))\n",
      "File \u001b[1;32mc:\\Users\\AubakirovMA\\Anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py:88\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[1;32m---> 88\u001b[0m     \u001b[39mraise\u001b[39;00m LinAlgError(\u001b[39m\"\u001b[39m\u001b[39mSingular matrix\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "# создадим вырожденную матрицу А\n",
    "A = np.array([\n",
    "    [1, 1, 1, 1], \n",
    "    [2, 1, 1, 2], \n",
    "    [-2, -1, -1, -2]]\n",
    ").T\n",
    "y = np.array([1, 2, 5, 1])\n",
    "# вычислим OLS-оценку для коэффициентов\n",
    "w_hat=np.linalg.inv(A.T@A)@A.T@y\n",
    "print(w_hat) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и ожидалось, мы получили ошибку, говорящую о том, что матрица $A^TA$ — сингулярная (вырожденная), а значит обратить её не получится. Что и требовалось доказать — с математикой всё сходится.\n",
    "\n",
    "⭐ Настало время фокусов!\n",
    "\n",
    "Попробуем обучить модель линейной регрессии `LinearRegression` из модуля `sklearn`, используя нашу вырожденную матрицу :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_hat: [ 6.   -1.25  1.25]\n"
     ]
    }
   ],
   "source": [
    "# создаём модель линейной регрессии\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "# вычисляем коэффициенты регрессии\n",
    "model.fit(A, y)\n",
    "print('w_hat:', model.coef_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Никакой ошибки не возникло! Более того, у нас даже получились вполне адекватные оценки коэффициентов линейной регрессии $\\hat{\\vec{w}}$.\n",
    "\n",
    "?Но ведь мы только что использовали формулу для вычисления коэффициентов при расчётах вручную и получали ошибку. Как мы могли получить результат, если матрица $A^TA$ вырожденная? Существование обратной матрицы для неё противоречит законам линейной алгебры. Неужели это очередной случай, когда «мнения» математики и Python расходятся?\n",
    "\n",
    "На самом деле, не совсем. Здесь нет никакой магии, ошибки округления или бага. Просто в реализации линейной регрессии в `sklearn` предусмотрена **борьба с плохо определёнными (близкими к вырожденным и вырожденными) матрицами**.\n",
    "\n",
    ">Для этого используется метод под названием **сингулярное разложение** (`SVD`). О нём мы будем говорить отдельно, однако уже сейчас отметим тот факт, что данный метод позволяет всегда получать корректные значения при обращении матриц.\n",
    ">\n",
    ">Если вы хотите понять, почему так происходит, ознакомьтесь с этой [статьёй](https://towardsdatascience.com/understanding-linear-regression-using-the-singular-value-decomposition-1f37fb10dd33).\n",
    ">\n",
    ">**Суть метода** заключается в том, что в `OLS`-формуле мы на самом деле используем не саму матрицу $A$, а её диагональное представление из сингулярного разложения, которое гарантированно является невырожденным. Вот и весь секрет.\n",
    "\n",
    "?Правда, открытым остаётся вопрос: **можно ли доверять коэффициентам**, полученным таким способом, и интерпретировать их? \n",
    "\n",
    "В дальнейшем мы увидим, что делать этого лучше не стоит: возможна такая ситуация, при которой коэффициенты при линейно зависимых факторах, которые получаются в результате применения линейной регрессии через сингулярное разложение, могут получиться слишком большими по модулю. Они могут измеряться миллионами, миллиардами и более высокими порядками, что не будет иметь отношения к действительности. Такие коэффициенты не подлежат интерпретации.\n",
    "\n",
    "Заметим, что в случае использования решения через сингулярное разложение для линейно зависимых столбцов коэффициенты будут всегда получаться одинаковыми по модулю, но различными по знаку: $w_1=-1.25$ и $w_2=1.25$. Неудивительно, ведь второй и третий столбцы матрицы $A$ линейно зависимы с коэффициентом $-1$.\n",
    "\n",
    "Запишем итоговое уравнение линейной регрессии:\n",
    "\n",
    "$y=w_{0} \\overrightarrow{1}+w_{1} \\vec{x}_{1}+w_{2} \\vec{x}_{2}=6-1.25 \\cdot \\vec{x}_{1}+1.25 \\cdot \\vec{x}_{2},$\n",
    "\n",
    "поставим столбцы матрицы $A$ в данное уравнение, чтобы получить прогноз:\n",
    "\n",
    "$\\vec{y}=6\\left(\\begin{array}{} 1\\\\1\\\\1\\\\1\\end{array}\\right)-1.25\\left(\\begin{array}{} 2\\\\1\\\\1\\\\2\\end{array}\\right)+1.25\\left(\\begin{array}{} -2\\\\-1\\\\-1\\\\-2\\end{array}\\right)=\\left(\\begin{array}{} 1\\\\3.5\\\\3.5\\\\1\\end{array}\\right)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Примечание**. На самом деле сингулярное разложение зашито в функцию `np.linalg.lstsq()`, которая позволяет в одну строку построить модель линейной регрессии по МНК:\n",
    ">\n",
    ">классическая `OLS`-регрессия в numpy с возможностью получения решения даже для вырожденных матриц\n",
    ">\n",
    ">`np.linalg.lstsq(A, y, rcond=None)`\n",
    ">\n",
    ">Функция возвращает четыре значения:\n",
    ">\n",
    ">* вектор рассчитанных коэффициентов линейной регрессии;\n",
    ">\n",
    ">* сумму квадратов ошибок, `MSE` (она не считается, если ранг матрицы $A$ меньше числа неизвестных, как в нашем случае);\n",
    ">\n",
    ">* ранг матрицы $A$;\n",
    ">\n",
    ">* вектор из сингулярных значений, которые как раз и оберегают нас от ошибки (о них мы поговорим позже).\n",
    ">\n",
    ">Обратите внимание, что мы получили те же коэффициенты, что и с помощью `sklearn`. При этом ранг матрицы $A$ равен 2, что меньше количества неизвестных коэффициентов. Это ожидаемо говорит о вырожденности матрицы $A$ и, как следствие, матрицы $A^TA$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "## Резюмируем ↓\n",
    "\n",
    "* Для поиска коэффициентов модели линейной регрессии используется МНК-оценка: \n",
    "\n",
    "$\\vec{w}=\\left(A^{T} A\\right)^{-1} A^{T} \\vec{y}$\n",
    "\n",
    "* Полученная матричная формула не зависит от размерности матрицы наблюдений $A$ и работает при любом количестве объектов/признаков в данных.\n",
    "\n",
    "* Для реализации обучения модели линейной регрессии по `МНК` в `sklearn` используется класс `LinearRegression`.\n",
    "\n",
    "* Для предотвращения обращения вырожденной матрицы $A$ в `LinearRegression` вместо самой матрицы используется её сингулярное разложение. Поэтому на практике при построении модели линейной регрессии вместо ручного вычисления обратной матрицы с помощью `np.inv()` приоритетнее пользоваться именно `LinearRegression` из `sklearn` (или `np.linalg.lstsq()`).\n",
    "\n",
    "Данный метод оберегает от ошибки только при обращении плохо обусловленных и вырожденных матриц и не гарантирует получение корректных коэффициентов линейной регрессии."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Стандартизация векторов и матрица корреляции <a class=\"anchor\" id=2></a>\n",
    "\n",
    "[к содержанию](#0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## СТАНДАРТИЗАЦИЯ ВЕКТОРОВ\n",
    "\n",
    "В модулях по разведывательному анализу данных и машинному обучению мы не раз говорили о преобразованиях признаков путём нормализации и стандартизации. Вспомним, что это такое ↓\n",
    "\n",
    ">**Нормализация** — это процесс приведения признаков к единому масштабу, например от 0 до 1. Пример — min-max-нормализация:\n",
    ">\n",
    ">$x_{scaled} =  \\frac{x - x_{min}}{x_{max} - x_{min}}$\n",
    ">\n",
    ">**Стандартизация** — это процесс приведения признаков к единому масштабу характеристик распределения — нулевому среднему и единичному стандартному отклонению:\n",
    ">\n",
    ">$x_{scaled} =  \\frac{x - x_{mean}}{x_{std}}$\n",
    "\n",
    "В линейной алгебре под стандартизацией вектора $\\vec{x} \\in \\mathbb{R}^n$ понимается несколько другая операция, которая проходит в два этапа:\n",
    "\n",
    "1. Центрирование вектора — это операция приведения среднего к 0:\n",
    "\n",
    "$\\vec{x}_{cent} = \\vec{x} - \\vec{x}_{mean}$\n",
    "\n",
    "2. Нормирование вектора — это операция приведения диапазона вектора к масштабу от -1 до 1 путём деления центрированного вектора на его длину:\n",
    "\n",
    "$\\vec{x}_{st} =  \\frac{\\vec{x}_{cent}}{ \\| \\vec{x}_{cent} \\| },$\n",
    "\n",
    "где $\\vec{x}_{mean}$ — вектор, составленный из среднего значения вектора $\\vec{x}$, а $\\| \\vec{x}_{cent} \\|$ — длина вектора  $\\vec{x}_{cent}$.\n",
    "\n",
    "В результате стандартизации вектора всегда получается новый вектор, длина которого равна 1:\n",
    "\n",
    "$\\| \\vec{x}_{st} \\|  = 1$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример № 1**\n",
    "\n",
    "Необходимо стандартизировать векторы:\n",
    "\n",
    "$\\vec{x_1}=\\left(\\begin{array}{} 1\\\\2\\\\3 \\end{array}\\right)$ и $\\vec{x_2}=\\left(\\begin{array}{} 3000\\\\1000\\\\2000 \\end{array}\\right)$\n",
    "\n",
    "Центрируем:\n",
    "\n",
    "$\\vec{x_1}=\\frac{1+2+3}{3}=3$\n",
    "\n",
    "$\\vec{x}_{1cent}=\\left(\\begin{array}{} 1\\\\2\\\\3 \\end{array}\\right)-\\left(\\begin{array}{} 3\\\\3\\\\3 \\end{array}\\right)=\\left(\\begin{array}{} -2\\\\-1\\\\3 \\end{array}\\right)$\n",
    "\n",
    "$\\vec{x_2}=\\frac{3000+1000+2000}{3}=2000$\n",
    "\n",
    "$\\vec{x}_{2cent}=\\left(\\begin{array}{} 3000\\\\1000\\\\2000 \\end{array}\\right)-\\left(\\begin{array}{} 2000\\\\2000\\\\2000 \\end{array}\\right)=\\left(\\begin{array}{} 1000\\\\-1000\\\\0 \\end{array}\\right)$\n",
    "\n",
    "Нормируем:\n",
    "\n",
    "$\\| \\vec{x}_{1cent} \\| =\\sqrt{(-2)^2 + (-1)^2 + 3^2}=\\sqrt{14}$\n",
    "\n",
    "$\\| \\vec{x}_{1st} \\| = \\frac{1}{\\| \\vec{x}_{1cent} \\|}\\cdot\\vec{x}_{1cent}=\\frac{1}{\\sqrt{14}}\\cdot\\left(\\begin{array}{} -2\\\\-1\\\\3 \\end{array}\\right)\\approx\\left(\\begin{array}{} -0.535\\\\-0.267\\\\0.802 \\end{array}\\right)$\n",
    "\n",
    "$. $\n",
    "\n",
    "$\\| \\vec{x}_{2cent} \\| =\\sqrt{(1000)^2 + (-1000)^2 + 0^2}=1000\\sqrt{2}$\n",
    "\n",
    "$\\| \\vec{x}_{2st} \\| = \\frac{1}{\\| \\vec{x}_{2cent} \\|}\\cdot\\vec{x}_{2cent}=\\frac{1}{1000\\sqrt{2}}\\cdot\\left(\\begin{array}{} 1000\\\\-1000\\\\0 \\end{array}\\right)\\approx\\left(\\begin{array}{} 0.707\\\\-0.707\\\\0 \\end{array}\\right)$\n",
    "\n",
    "Как видите, теперь оба признака имеют значения от -1 до 1 и равный порядок, в отличие от исходных признаков.\n",
    "\n",
    "Давайте посмотрим, что произойдёт с матрицей Грама после стандартизации векторов $\\vec{x}_1$ и $\\vec{x}_2$:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример № 2**\n",
    "\n",
    "Найти матрицу для стандартизированных признаков для\n",
    "\n",
    "$\\vec{x_1}=\\left(\\begin{array}{} 1\\\\2\\\\3 \\end{array}\\right)$ и $\\vec{x_2}=\\left(\\begin{array}{} 3000\\\\1000\\\\2000 \\end{array}\\right)$\n",
    "\n",
    "Вычислим попарные скалярные произведения новых признаков:\n",
    "\n",
    "$G(\\vec{x}_{1,st},\\vec{x}_{2,st})=G(\\left(\\begin{array}{} -0.535\\\\-0.267\\\\0.802 \\end{array}\\right),\\left(\\begin{array}{} 0.707\\\\-0.707\\\\0 \\end{array}\\right))=\\left(\\begin{array}{} (\\vec{x}_{1,st},\\vec{x}_{1,st}) & (\\vec{x}_{1,st},\\vec{x}_{2,st}) \\\\ (\\vec{x}_{2,st},\\vec{x}_{1,st}) & (\\vec{x}_{2,st},\\vec{x}_{2,st})  \\end{array}\\right)=\\left(\\begin{array}{} 1 & -0.189 \\\\ -0.189 & 1 \\end{array}\\right)$\n",
    "\n",
    "Как видите, все числа — в диапазоне от -1 до 1. \n",
    "\n",
    ">Забегая вперёд, скажем, что это так называемые **выборочные корреляции признаков**, а сама матрица является **матрицей корреляций** или **корреляционной матрицей**. Пока просто запомните, как выглядит эта матрица.\n",
    "\n",
    "Вот **ещё одна особенность стандартизации** ↓\n",
    "\n",
    "До стандартизации мы прогоняли регрессию $y$ на регрессоры $x_1, x_2, …, x_k$ и константу. Всего получалось $k+1$ коэффициентов:\n",
    "\n",
    "$\\vec{y}=w_{0}+w_{1} \\vec{x}_{1}+w_{2} \\vec{x}_{2}+\\ldots+w_{k} \\vec{x}_{k}$\n",
    "\n",
    "После стандартизации мы прогоняем регрессию стандартизованного $y$ на стандартизованные регрессоры **без константы**:\n",
    "\n",
    "$\\vec{y}=w_{1_{st}} \\vec{x}_{1_{st}}+w_{2_{st}} \\vec{x}_{2_{st}}+\\ldots+w_{k_{st}} \\vec{x}_{k_{st}}$\n",
    "\n",
    "Математически мы получим одну и ту же регрессию в том смысле, что если пересчитать стандартизированные коэффициенты, мы получим исходные. То же и с прогнозом (пересчёт здесь опустим).\n",
    "\n",
    "***\n",
    "\n",
    "**В ЧЁМ БОНУСЫ?**\n",
    "\n",
    "Математика говорит, что регрессия исходного $y$ на исходные («сырые») признаки c константой точно такая же, как регрессия стандартизированного на стандартизированные признаки без константы. В чём же разница? Математически — ни в чём.\n",
    "\n",
    "На прогноз модели линейной регрессии, построенной по МНК, и её качество стандартизация практически не влияет. Масштабы признаков будут иметь значение только в том случае, если для поиска коэффициентов вы используете численные методы, такие как градиентный спуск (`SGDRegressor` из `sklearn`). О нём мы поговорим, когда будем знакомиться с алгоритмом градиентного спуска в модуле по оптимизации.\n",
    "\n",
    "Однако с точки зрения интерпретации важности коэффициентов разница есть. Если вы занимаетесь отбором наиболее важных признаков по значению коэффициентов линейной регрессии на нестандартизированных данных, это будет не совсем корректно: один признак может изменяться от 0 до 1, а второй — от -1000 до 1000. Коэффициенты при них также будут различного масштаба. Если же вы посмотрите оценки коэффициентов регрессии после стандартизации, то они будут в едином масштабе, что даст более цельную и объективную картину.\n",
    "\n",
    "Более важный бонус заключается в том, что после стандартизации матрица Грама признаков как по волшебству превращается в корреляционную матрицу, о которой пойдёт речь далее. Почему это хорошо? На свойства корреляционной матрицы опираются такие алгоритмы, как метод главных компонент и сингулярное разложение, а так как «сырая» и стандартизированная регрессия математически эквивалентны, то имеет смысл исследовать стандартизированную, а результаты обобщить на «сырую»."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример № 3**\n",
    "\n",
    "Вновь рассмотрим данные о стоимости жилья в районах Бостона.\n",
    "\n",
    "На этот раз возьмём четыре признака: `CHAS`, `LSTAT`, `CRIM` и `RM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHAS</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>CRIM</th>\n",
       "      <th>RM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.069170</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>3.613524</td>\n",
       "      <td>6.284634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.253994</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>8.601545</td>\n",
       "      <td>0.702617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>0.006320</td>\n",
       "      <td>3.561000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>0.082045</td>\n",
       "      <td>5.885500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>0.256510</td>\n",
       "      <td>6.208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>3.677083</td>\n",
       "      <td>6.623500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>88.976200</td>\n",
       "      <td>8.780000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CHAS       LSTAT        CRIM          RM\n",
       "count  506.000000  506.000000  506.000000  506.000000\n",
       "mean     0.069170   12.653063    3.613524    6.284634\n",
       "std      0.253994    7.141062    8.601545    0.702617\n",
       "min      0.000000    1.730000    0.006320    3.561000\n",
       "25%      0.000000    6.950000    0.082045    5.885500\n",
       "50%      0.000000   11.360000    0.256510    6.208500\n",
       "75%      0.000000   16.955000    3.677083    6.623500\n",
       "max      1.000000   37.970000   88.976200    8.780000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_data[['CHAS', 'LSTAT', 'CRIM','RM']].describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что каждый из признаков измеряется в различных единицах и изменяется в различных диапазонах: например, `CHAS` лежит в диапазоне от 0 до 1, а вот `CRIM` — в диапазоне от 0.006 до 88.976.\n",
    "\n",
    "Рассмотрим модель линейной регрессии по МНК без стандартизации. Помним, что необходимо добавить столбец из единиц:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.92052548]\n",
      " [ 3.9975594 ]\n",
      " [-0.58240212]\n",
      " [-0.09739445]\n",
      " [ 5.07554248]]\n"
     ]
    }
   ],
   "source": [
    "# составляем матрицу наблюдений и вектор целевой переменной\n",
    "A = np.column_stack((np.ones(506), boston_data[['CHAS', 'LSTAT', 'CRIM','RM']]))\n",
    "y = boston_data[['PRICE']]\n",
    "# вычисляем OLS-оценку для коэффициентов без стандартизации\n",
    "w_hat=np.linalg.inv(A.T@A)@A.T@y\n",
    "print(w_hat.values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот наши коэффициенты. Округлим их для наглядности:\n",
    "\n",
    "$\\hat{w}_0 = -1.92$\n",
    "\n",
    "$\\hat{w}_{CHAS} = 4$\n",
    "\n",
    "$\\hat{w}_{LSTAT} = -0.6$\n",
    "\n",
    "$\\hat{w}_{CRIM} = -0.1$\n",
    "\n",
    "$\\hat{w}_{RM} = 5$\n",
    "\n",
    "Давайте вспомним интерпретацию коэффициентов построенной модели линейной регрессии, которую мы изучали в модуле «ML-2. Обучение с учителем: регрессия». Значение коэффициента $\\hat{w}_i$ означает, на сколько в среднем изменится медианная цена (в тысячах долларов) при увеличении $x_i$ на 1.\n",
    "\n",
    "Например, если количество низкостатусного населения (`LSTAT`) увеличится на 1 %, то медианная цена домов в районе (в среднем) упадёт на 0.1 тысяч долларов. А если среднее количество комнат (`RM`) в районе станет больше на 1, то медианная стоимость домов в районе (в среднем) увеличится на 5 тысяч долларов. \n",
    "\n",
    ">Тут в голову может прийти мысль: судя по значению коэффициентов, количество комнат (`RM`) оказывает на стоимость жилья большее влияние, чем процент низкостатусного населения (`LSTAT`). Однако **такой вывод будет ошибочным**. Мы не учитываем, что признаки, а значит и коэффициенты линейной регрессии, лежат в разных масштабах. Чтобы говорить о важности влияния признаков на модель, нужно строить её на стандартизированных данных.\n",
    "\n",
    "Помним, что для построения стандартизированной линейной регрессии нам не нужен вектор свободных коэффициентов, а значит и столбец из единиц тоже не понадобится.\n",
    "\n",
    "Сначала центрируем векторы, которые находятся в столбцах матрицы $A$. Для этого вычтем среднее, вычисленное по строкам матрицы $A$ в каждом столбце, с помощью метода `mean()`. Затем разделим результат на длины центрированных векторов, вычисленных с помощью функции `linalg.norm()`.\n",
    "\n",
    ">**Примечание**. Обратите внимание, что для функции `linalg.norm()` обязательно необходимо указать параметр `axis=0`, так как по умолчанию норма считается для всей матрицы, а не для каждого столбца в отдельности. С определением нормы матрицы и тем, как она считается, вы можете ознакомиться в [документации к функции norm()](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHAS</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>CRIM</th>\n",
       "      <th>RM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.00</td>\n",
       "      <td>506.00</td>\n",
       "      <td>506.00</td>\n",
       "      <td>506.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         CHAS   LSTAT    CRIM      RM\n",
       "count  506.00  506.00  506.00  506.00\n",
       "mean    -0.00   -0.00    0.00   -0.00\n",
       "std      0.04    0.04    0.04    0.04\n",
       "min     -0.01   -0.07   -0.02   -0.17\n",
       "25%     -0.01   -0.04   -0.02   -0.03\n",
       "50%     -0.01   -0.01   -0.02   -0.00\n",
       "75%     -0.01    0.03    0.00    0.02\n",
       "max      0.16    0.16    0.44    0.16"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# составляем матрицу наблюдений без дополнительного столбца из единиц\n",
    "A = boston_data[['CHAS', 'LSTAT', 'CRIM','RM']]\n",
    "y = boston_data[['PRICE']]\n",
    "# стандартизируем векторы в столбцах матрицы A\n",
    "A_cent = A - A.mean()\n",
    "A_st = A_cent/np.linalg.norm(A_cent, axis=0)\n",
    "A_st.describe().round(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь векторы имеют одинаковые средние значения и стандартные отклонения. Если вычислить длину каждого из векторов, мы увидим, что они будут равны 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(A_st, axis=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для получения стандартизированных коэффициентов нам также понадобится стандартизация целевой переменной $y$ по тому же принципу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# стандартизируем вектор целевой переменной\n",
    "y_cent = y - y.mean()\n",
    "y_st = y_cent/np.linalg.norm(y_cent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формула для вычисления коэффициента та же, что и раньше, только матрица $A$ теперь заменяется на $A_{st}$, а $y$ — на $y_{st}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.11039956]\n",
      " [-0.45220423]\n",
      " [-0.09108766]\n",
      " [ 0.38774848]]\n"
     ]
    }
   ],
   "source": [
    "# вычислим OLS-оценку для стандартизированных коэффициентов\n",
    "w_hat_st=np.linalg.inv(A_st.T@A_st)@A_st.T@y_st\n",
    "print(w_hat_st.values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вновь смотрим на коэффициенты. Помним, что коэффициента $\\hat{w}_0$ у нас больше нет:\n",
    "\n",
    "$\\hat{w}_{CHAS} = 0.11$\n",
    "\n",
    "$\\hat{w}_{LSTAT} = -0.45$\n",
    "\n",
    "$\\hat{w}_{CRIM} = -0.09$\n",
    "\n",
    "$\\hat{w}_{RM} = 0.38$\n",
    "\n",
    "Итак, мы видим картину, прямо противоположную той, что видели ранее. Теперь модуль коэффициента $\\left|\\hat{w}_{LSTAT, \\ st} \\right| = 0.45$ будет выше, чем модуль коэффициента $\\left|\\hat{w}_{RM, \\ st} \\right| = 0.38$. Значит, процент низкостатусного населения оказывает большее влияние на значение стоимости жилья, чем количество комнат.\n",
    "\n",
    "Однако теперь интерпретировать сами коэффициенты в тех же измерениях у нас не получится.\n",
    "\n",
    "***\n",
    "\n",
    "**Сделаем важный вывод** ↓\n",
    "\n",
    "Для того чтобы проинтерпретировать оценки коэффициентов линейной регрессии (понять, каков будет прирост целевой переменной при изменении фактора на 1 условную единицу), нам достаточно построить линейную регрессию в обычном виде без стандартизации и получить обычный вектор $\\hat{\\vec{w}}$.\n",
    "\n",
    "Однако, чтобы корректно говорить о том, какой фактор оказывает на прогноз большее влияние, необходимо рассматривать стандартизированную оценку вектора коэффициентов $\\hat{\\vec{w}}_{st}$.\n",
    "\n",
    "Давайте поближе взглянем на матрицу Грама для стандартизированных факторов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHAS</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>CRIM</th>\n",
       "      <th>RM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CHAS</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.053929</td>\n",
       "      <td>-0.055892</td>\n",
       "      <td>0.091251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTAT</th>\n",
       "      <td>-0.053929</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.455621</td>\n",
       "      <td>-0.613808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRIM</th>\n",
       "      <td>-0.055892</td>\n",
       "      <td>0.455621</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.219247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RM</th>\n",
       "      <td>0.091251</td>\n",
       "      <td>-0.613808</td>\n",
       "      <td>-0.219247</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           CHAS     LSTAT      CRIM        RM\n",
       "CHAS   1.000000 -0.053929 -0.055892  0.091251\n",
       "LSTAT -0.053929  1.000000  0.455621 -0.613808\n",
       "CRIM  -0.055892  0.455621  1.000000 -0.219247\n",
       "RM     0.091251 -0.613808 -0.219247  1.000000"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# матрица Грама\n",
    "A_st.T @ A_st"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На самом деле мы с вами только что вычислили **матрицу выборочных корреляций** наших исходных факторов. Мы уже сталкивались с ней много раз в разделах по разведывательному анализу данных и машинному обучению, правда, вычисляли её мы с помощью функции `Pandas`, а теперь научились делать это вручную.\n",
    "\n",
    ">**Примечание**. Матрицу корреляций можно получить только в том случае, если производить стандартизацию признаков как векторы (делить на длину центрированного вектора $\\vec{x}_{st}$). Другие способы стандартизации/нормализации признаков не превращают матрицу Грама в матрицу корреляций."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Практика. Лин.регрессия МНК <a class=\"anchor\" id=2></a>\n",
    "\n",
    "[к содержанию](#0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Полиноминальная регрессия <a class=\"anchor\" id=2></a>\n",
    "\n",
    "[к содержанию](#0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Регуляция <a class=\"anchor\" id=2></a>\n",
    "\n",
    "[к содержанию](#0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Практика. Полиноминальная регрессия и регуляция <a class=\"anchor\" id=2></a>\n",
    "\n",
    "[к содержанию](#0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Итоги <a class=\"anchor\" id=2></a>\n",
    "\n",
    "[к содержанию](#0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c3380a37b4678e1f5e651331348d62bc6038aef0d5f414da260f404a34792558"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
